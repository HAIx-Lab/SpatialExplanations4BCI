{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c32cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split,cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score,balanced_accuracy_score #roc_auc_score,precision_score,recall_score,f1_score,classification_report\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "import pyriemann\n",
    "import ot\n",
    "\n",
    "import myimporter\n",
    "from BCI_functions import *  # BCI_functions.ipynb contains some functions we might use multiple times in this tutorial\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b453688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: This class if has list of subject id can later support combination of sub ids\n",
    "# TODO: add a function transform to convert dataset to train test, avoiding repetition of same code\n",
    "\n",
    "class EEGMMIDTrSet(Data.Dataset):\n",
    "    def __init__(self, subject_id, transform=None):\n",
    "        root_dir = \"../Deep-Learning-for-BCI/dataset/\"\n",
    "        dataset_raw = np.load(root_dir + str(subject_id) + '.npy')\n",
    "        dataset=[]  # feature after filtering\n",
    "\n",
    "        # EEG Gamma pattern decomposition\n",
    "        for i in range(dataset_raw[:,:-1].shape[1]):\n",
    "            x = dataset_raw[:, i]\n",
    "            fs = 160.0\n",
    "            lowcut = 8.0\n",
    "            highcut = 30.0\n",
    "            y = butter_bandpass_filter(x, lowcut, highcut, fs, order=3)\n",
    "            dataset.append(y)\n",
    "        dataset=np.array(dataset).T\n",
    "        dataset=np.hstack((dataset,dataset_raw[:,-1:]))\n",
    "        print(dataset.shape)\n",
    "        # keep 4,5 which are left and right fist open close imagery classes, remove rest\n",
    "        # refer 1-Data.ipynb for the details\n",
    "        removed_label = [0,1,6,7,8,9,10]  # [0,1,2,3,4,5,10] for hf # [0,1,6,7,8,9,10] for lr\n",
    "        for ll in removed_label:\n",
    "            id = dataset[:, -1]!=ll\n",
    "            dataset = dataset[id]\n",
    "\n",
    "        # Pytorch needs labels to be sequentially ordered starting from 0\n",
    "        dataset[:, -1][dataset[:, -1] == 2] = 0\n",
    "        dataset[:, -1][dataset[:, -1] == 4] = 0\n",
    "        dataset[:, -1][dataset[:, -1] == 3] = 1\n",
    "        dataset[:, -1][dataset[:, -1] == 5] = 1\n",
    "#         dataset[:, -1][dataset[:, -1] == 10] = 2\n",
    "        \n",
    "        # data segmentation\n",
    "        n_class = 2 #int(11-len(removed_label))  # 0~9 classes ('10:rest' is not considered)\n",
    "        no_feature = 64  # the number of the features\n",
    "        segment_length = 160 #160  # selected time window; 16=160*0.1\n",
    "        \n",
    "        #Overlapping is removed to avoid training set overlap with test set\n",
    "        data_seg = extract(dataset, n_classes=n_class, n_fea=no_feature, \n",
    "                           time_window=segment_length, moving=(segment_length))  # /2 for 50% overlapping\n",
    "        print('After segmentation, the shape of the data:', data_seg.shape)\n",
    "\n",
    "        # split training and test data\n",
    "        no_longfeature = no_feature*segment_length\n",
    "        data_seg_feature = data_seg[:, :no_longfeature]\n",
    "        self.data_seg_label = data_seg[:, no_longfeature:no_longfeature+1]\n",
    "        \n",
    "        # Its important to have random state set equal for Training and test dataset\n",
    "        train_feature, test_feature, train_label, test_label = train_test_split(\n",
    "            data_seg_feature, self.data_seg_label,random_state=0, shuffle=True,stratify=self.data_seg_label)\n",
    "\n",
    "        # Check the class label splits to maintain balance\n",
    "        unique, counts = np.unique(self.data_seg_label, return_counts=True)\n",
    "        left_perc = counts[0]/sum(counts)\n",
    "        if left_perc < 0.4 or left_perc > 0.6:\n",
    "            print(\"Imbalanced dataset with split of: \",left_perc,1-left_perc)\n",
    "        else:\n",
    "            print(\"Classes balanced.\")\n",
    "        unique, counts = np.unique(train_label, return_counts=True)\n",
    "        print(\"Class label splits in training set \\n \",np.asarray((unique, counts)).T)\n",
    "        unique, counts = np.unique(test_label, return_counts=True)\n",
    "        print(\"Class label splits in test set\\n \",np.asarray((unique, counts)).T)\n",
    "\n",
    "\n",
    "\n",
    "        # normalization\n",
    "        # before normalize reshape data back to raw data shape\n",
    "        train_feature_2d = train_feature.reshape([-1, no_feature])\n",
    "        test_feature_2d = test_feature.reshape([-1, no_feature])\n",
    "\n",
    "        scaler1 = StandardScaler().fit(train_feature_2d)\n",
    "        train_fea_norm1 = scaler1.transform(train_feature_2d) # normalize the training data\n",
    "        test_fea_norm1 = scaler1.transform(test_feature_2d) # normalize the test data\n",
    "        print('After normalization, the shape of training feature:', train_fea_norm1.shape,\n",
    "              '\\nAfter normalization, the shape of test feature:', test_fea_norm1.shape)\n",
    "        \n",
    "        # after normalization, reshape data to 3d\n",
    "        train_fea_norm1 = train_fea_norm1.reshape([-1, segment_length, no_feature])\n",
    "        test_fea_norm1 = test_fea_norm1.reshape([-1, segment_length, no_feature])\n",
    "        print('After reshape, the shape of training feature:', train_fea_norm1.shape,\n",
    "              '\\nAfter reshape, the shape of test feature:', test_fea_norm1.shape)\n",
    "        \n",
    "        # reshape for data shape: (trial, conv channel, electrode channel, time samples)\n",
    "        # earlier it was (trial,timesamples,electrode_channel)\n",
    "        train_fea_reshape1 = np.swapaxes(train_fea_norm1,1,2)\n",
    "        test_fea_reshape1 = np.swapaxes(test_fea_norm1,1,2)\n",
    "        print('After expand dims, the shape of training feature:', train_fea_reshape1.shape,\n",
    "              '\\nAfter expand dims, the shape of test feature:', test_fea_reshape1.shape)\n",
    "        \n",
    "        self.data = train_fea_reshape1 # torch.tensor(train_fea_reshape1)\n",
    "        self.targets = train_label.flatten() #torch.tensor(train_label.flatten()).long()\n",
    "        \n",
    "        print(\"data and target type:\",type(self.data),type(self.targets))\n",
    "\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data, target = self.data[idx], self.targets[idx]\n",
    "        return data, target\n",
    "    \n",
    "    def get_class_weights(self):\n",
    "        class_weights=class_weight.compute_class_weight('balanced',np.unique(self.data_seg_label),\n",
    "                                                        self.data_seg_label[:,0])\n",
    "        return class_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ef5445",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGMMIDTsSet(Data.Dataset):\n",
    "    def __init__(self, subject_id, transform=None):\n",
    "        root_dir = \"../Deep-Learning-for-BCI/dataset/\"\n",
    "#         dataset = np.load(root_dir + str(subject_id) + '.npy')\n",
    "        dataset_raw = np.load(root_dir + str(subject_id) + '.npy')\n",
    "        dataset=[]  # feature after filtering\n",
    "\n",
    "        # EEG Gamma pattern decomposition\n",
    "        for i in range(dataset_raw[:,:-1].shape[1]):\n",
    "            x = dataset_raw[:, i]\n",
    "            fs = 160.0\n",
    "            lowcut = 8.0\n",
    "            highcut = 30.0\n",
    "            y = butter_bandpass_filter(x, lowcut, highcut, fs, order=3)\n",
    "            dataset.append(y)\n",
    "        dataset=np.array(dataset).T\n",
    "        dataset=np.hstack((dataset,dataset_raw[:,-1:]))\n",
    "        # keep 4,5 which are left and right fist open close imagery classes, remove rest\n",
    "        # refer 1-Data.ipynb for the details\n",
    "        removed_label = [0,1,6,7,8,9,10]  # [0,1,2,3,4,5,10] for hf # [0,1,6,7,8,9,10] for lr\n",
    "        for ll in removed_label:\n",
    "            id = dataset[:, -1]!=ll\n",
    "            dataset = dataset[id]\n",
    "\n",
    "        # Pytorch needs labels to be sequentially ordered starting from 0\n",
    "        dataset[:, -1][dataset[:, -1] == 2] = 0\n",
    "        dataset[:, -1][dataset[:, -1] == 4] = 0\n",
    "        dataset[:, -1][dataset[:, -1] == 3] = 1\n",
    "        dataset[:, -1][dataset[:, -1] == 5] = 1\n",
    "        \n",
    "        # data segmentation\n",
    "        n_class = 2 #int(11-len(removed_label))  # 0~9 classes ('10:rest' is not considered)\n",
    "        no_feature = 64  # the number of the features\n",
    "        segment_length = 160 #160  # selected time window; 16=160*0.1\n",
    "        \n",
    "        #Overlapping is removed to avoid training set overlap with test set\n",
    "        data_seg = extract(dataset, n_classes=n_class, n_fea=no_feature, \n",
    "                           time_window=segment_length, moving=(segment_length))  # /2 for 50% overlapping\n",
    "        print('After segmentation, the shape of the data:', data_seg.shape)\n",
    "\n",
    "        # split training and test data\n",
    "        no_longfeature = no_feature*segment_length\n",
    "        data_seg_feature = data_seg[:, :no_longfeature]\n",
    "        data_seg_label = data_seg[:, no_longfeature:no_longfeature+1]\n",
    "        # Its important to have random state set equal for Training and test dataset\n",
    "        train_feature, test_feature, train_label, test_label = train_test_split(\n",
    "            data_seg_feature, data_seg_label,random_state=0, shuffle=True,stratify=data_seg_label)\n",
    "\n",
    "        # Check the class label splits to maintain balance\n",
    "        unique, counts = np.unique(data_seg_label, return_counts=True)\n",
    "        left_perc = counts[0]/sum(counts)\n",
    "        if left_perc < 0.4 or left_perc > 0.6:\n",
    "            print(\"Imbalanced dataset with split of: \",left_perc,1-left_perc)\n",
    "        else:\n",
    "            print(\"Classes balanced.\")\n",
    "        unique, counts = np.unique(train_label, return_counts=True)\n",
    "        print(\"Class label splits in training set \\n \",np.asarray((unique, counts)).T)\n",
    "        unique, counts = np.unique(test_label, return_counts=True)\n",
    "        print(\"Class label splits in test set\\n \",np.asarray((unique, counts)).T)\n",
    "\n",
    "\n",
    "\n",
    "        # normalization\n",
    "        # before normalize reshape data back to raw data shape\n",
    "        train_feature_2d = train_feature.reshape([-1, no_feature])\n",
    "        test_feature_2d = test_feature.reshape([-1, no_feature])\n",
    "\n",
    "        scaler1 = StandardScaler().fit(train_feature_2d)\n",
    "        train_fea_norm1 = scaler1.transform(train_feature_2d) # normalize the training data\n",
    "        test_fea_norm1 = scaler1.transform(test_feature_2d) # normalize the test data\n",
    "        print('After normalization, the shape of training feature:', train_fea_norm1.shape,\n",
    "              '\\nAfter normalization, the shape of test feature:', test_fea_norm1.shape)\n",
    "        \n",
    "        # after normalization, reshape data to 3d\n",
    "        train_fea_norm1 = train_fea_norm1.reshape([-1, segment_length, no_feature])\n",
    "        test_fea_norm1 = test_fea_norm1.reshape([-1, segment_length, no_feature])\n",
    "        print('After reshape, the shape of training feature:', train_fea_norm1.shape,\n",
    "              '\\nAfter reshape, the shape of test feature:', test_fea_norm1.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # reshape for data shape: (trial, conv channel, electrode channel, time samples)\n",
    "        # earlier it was (trial,timesamples,electrode_channel)\n",
    "        train_fea_reshape1 = np.swapaxes(train_fea_norm1,1,2)\n",
    "        test_fea_reshape1 = np.swapaxes(test_fea_norm1,1,2)\n",
    "        print('After expand dims, the shape of training feature:', train_fea_reshape1.shape,\n",
    "              '\\nAfter expand dims, the shape of test feature:', test_fea_reshape1.shape)\n",
    "        \n",
    "        self.data =  test_fea_reshape1#torch.tensor(test_fea_reshape1)\n",
    "        self.targets = test_label.flatten() #torch.tensor(test_label.flatten()).long()\n",
    "        \n",
    "        print(\"data and target type:\",type(self.data),type(self.targets))\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data, target = self.data[idx], self.targets[idx]\n",
    "        return data, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee6d500",
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_instance = pyriemann.channelselection.ElectrodeSelection(nelec=16, metric='riemann', n_jobs=1)\n",
    "ES_instance = ES_instance.fit(cov_64, y=labels, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f5a651",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_16 = ES_instance.transform(cov_64)\n",
    "cov_16.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5d4c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_mi_21 = pyriemann.estimation.Covariances().transform(train_ds[:][0][:,:21,:])\n",
    "cov_mi_21.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63dcc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_instance.subelec_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb22ff69",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdm = pyriemann.classification.MDM(metric=dict(mean='riemann', distance='riemann'))\n",
    "# cross validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "# Use scikit-learn Pipeline with cross_val_score function\n",
    "scores = cross_val_score(mdm, cov_16, labels, cv=cv, n_jobs=1)\n",
    "\n",
    "# Printing the results\n",
    "class_balance = np.mean(labels == labels[0])\n",
    "class_balance = max(class_balance, 1. - class_balance)\n",
    "print(\"MDM Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                              class_balance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a1fe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mdm = pyriemann.classification.MDM(metric=dict(mean='riemann', distance='riemann'))\n",
    "# cross validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "# Use scikit-learn Pipeline with cross_val_score function\n",
    "scores = cross_val_score(mdm, cov_mi_21, labels, cv=cv, n_jobs=1)\n",
    "\n",
    "# Printing the results\n",
    "class_balance = np.mean(labels == labels[0])\n",
    "class_balance = max(class_balance, 1. - class_balance)\n",
    "print(\"MDM Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                              class_balance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b109d016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_acc(confusion_matrix, class_id):\n",
    "    \"\"\"\n",
    "    confusion matrix of multi-class classification\n",
    "    \n",
    "    class_id: id of a particular class \n",
    "    \n",
    "    \"\"\"\n",
    "    confusion_matrix = np.float64(confusion_matrix)\n",
    "    TP = confusion_matrix[class_id,class_id]\n",
    "    FN = np.sum(confusion_matrix[class_id]) - TP\n",
    "    FP = np.sum(confusion_matrix[:,class_id]) - TP\n",
    "    TN = np.sum(confusion_matrix) - TP - FN - FP\n",
    "    print(\"for class id: \",class_id)\n",
    "    print(f\"TP: {TP}, FN: {FN}, FP: {FP}, TN: {TN} \")\n",
    "    \n",
    "    # sensitivity = 0 if TP == 0\n",
    "    if TP != 0:\n",
    "        sensitivity = TP/(TP+FN)\n",
    "    else:\n",
    "        sensitivity = 0.\n",
    "    \n",
    "    specificity = TN/(TN+FP)\n",
    "    accuracy = (TP+TN)/(TP+FP+FN+TN)\n",
    "    \n",
    "    return sensitivity, specificity, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a642c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_eegmmid(task_type, strat, sub_id, i=\"\"):\n",
    "    \n",
    "    start = time.time()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    train_ds = EEGMMIDTrSet(subject_id=sub_id)\n",
    "    test_ds = EEGMMIDTsSet(subject_id=sub_id)\n",
    "    \n",
    "    # compute covariance matrices on training data\n",
    "    cov_64 = pyriemann.estimation.Covariances('oas').transform(train_ds[:][0])\n",
    "    labels = train_ds[:][1]\n",
    "    \n",
    "    mdm = pyriemann.classification.MDM(metric=dict(mean='riemann', distance='riemann'))\n",
    "    \n",
    "    # cross validation\n",
    "    cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    # Use scikit-learn Pipeline with cross_val_score function\n",
    "    scores = cross_val_score(mdm, cov_64, labels, cv=cv, n_jobs=1)\n",
    "    \n",
    "    \n",
    "    cov_64_test = pyriemann.estimation.Covariances('oas').transform(test_ds[:][0])\n",
    "    mdm = mdm.fit(cov_64, labels)\n",
    "    y_pred = mdm.predict(cov_64_test)\n",
    "    cm = confusion_matrix(test_ds[:][1], y_pred)\n",
    "    \n",
    "    acc_0 = get_class_acc(cm,0)\n",
    "    acc_1 = get_class_acc(cm,1)\n",
    "    acc = accuracy_score(test_ds[:][1], y_pred)\n",
    "\n",
    "    # Printing the results\n",
    "    class_balance = np.mean(labels == labels[0])\n",
    "    class_balance = max(class_balance, 1. - class_balance)\n",
    "    print(\"train acc: \",scores)\n",
    "    print(\"test acc: \",acc, acc_0, acc_1)\n",
    "    print(\"chance level acc: \",class_balance)\n",
    "    \n",
    "    results = []\n",
    "    thisresults = []\n",
    "\n",
    "    print(i + \".\")\n",
    "    \n",
    "    \n",
    "    thisresults.append({\"task_type\":task_type,\n",
    "                                \"strategy\":strat,\n",
    "                                \"sub_id\":sub_id,\n",
    "                                \"iteration\":i,\n",
    "                                \"chance level acc\":class_balance,\n",
    "                                \"acc\":acc,\n",
    "                                \"acc0\":acc_0[0],\n",
    "                                \"acc1\":acc_1[0] })\n",
    "    results.append({\"task_type\":task_type,\n",
    "                    \"strategy\":strat,\n",
    "                    \"sub_id\":sub_id,\n",
    "                    \"iteration\":i,\n",
    "                    \"results\":thisresults})\n",
    "    elapsed = time.time() - start\n",
    "#     results.append({\"time\":elapsed})\n",
    "    with open(\"./results/riem/eegmmid_ws_\" + strat +\"_\"+ str(sub_id)+ \"_results\" + i + \".pkl\", \"wb\") as outfile:\n",
    "        pickle.dump(results, outfile)\n",
    "    print(\"\\t\" + str(elapsed) + \" seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83edeb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s_id in range(1,110):#[7,15,29,32,35,42,43,46,48,49,54,56,62,93,94,108]: #range(7,18):\n",
    "    print(\"\\n --------------------------------------------------- \\n\")\n",
    "    print(\"Starting for subject id:\",s_id)\n",
    "    for itr in range(1):\n",
    "        train_eegmmid(task_type=\"within_sub\",strat=\"riem\", sub_id=s_id, i=str(itr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
