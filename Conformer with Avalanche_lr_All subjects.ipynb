{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c32cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from avalanche.benchmarks.generators import ni_benchmark\n",
    "from avalanche.benchmarks.utils import AvalancheDataset\n",
    "from avalanche.models import SimpleMLP\n",
    "from avalanche.training.supervised import Naive, Cumulative, LwF, EWC, JointTraining, GEM, Replay\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from avalanche.evaluation.metrics import forgetting_metrics, accuracy_metrics,class_accuracy_metrics, loss_metrics, timing_metrics, cpu_usage_metrics, confusion_matrix_metrics, disk_usage_metrics, gpu_usage_metrics\n",
    "from avalanche.training.plugins import EvaluationPlugin, EarlyStoppingPlugin\n",
    "from avalanche.logging import InteractiveLogger, TextLogger, TensorboardLogger\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score,precision_score,recall_score,f1_score,classification_report\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import myimporter\n",
    "from BCI_functions import *  # BCI_functions.ipynb contains some functions we might use multiple times in this tutorial\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b453688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: This class if has list of subject id can later support combination of sub ids\n",
    "# TODO: add a function transform to convert dataset to train test, avoiding repetition of same code\n",
    "\n",
    "class EEGMMIDTrSet(Data.Dataset):\n",
    "    def __init__(self, subject_id, transform=None):\n",
    "        root_dir = \"../Deep-Learning-for-BCI/dataset/\"\n",
    "        dataset_raw = np.load(root_dir + str(subject_id) + '.npy')\n",
    "        dataset=[]  # feature after filtering\n",
    "\n",
    "        # EEG Gamma pattern decomposition\n",
    "        for i in range(dataset_raw[:,:-1].shape[1]):\n",
    "            x = dataset_raw[:, i]\n",
    "            fs = 160.0\n",
    "            lowcut = 8.0\n",
    "            highcut = 30.0\n",
    "            y = butter_bandpass_filter(x, lowcut, highcut, fs, order=3)\n",
    "            dataset.append(y)\n",
    "        dataset=np.array(dataset).T\n",
    "        dataset=np.hstack((dataset,dataset_raw[:,-1:]))\n",
    "        print(dataset.shape)\n",
    "        # keep 4,5 which are left and right fist open close imagery classes, remove rest\n",
    "        # refer 1-Data.ipynb for the details\n",
    "        removed_label = [0,1,6,7,8,9,10]  # [0,1,2,3,4,5,10] for hf # [0,1,6,7,8,9,10] for lr\n",
    "        for ll in removed_label:\n",
    "            id = dataset[:, -1]!=ll\n",
    "            dataset = dataset[id]\n",
    "\n",
    "        # Pytorch needs labels to be sequentially ordered starting from 0\n",
    "        dataset[:, -1][dataset[:, -1] == 2] = 0\n",
    "        dataset[:, -1][dataset[:, -1] == 4] = 0\n",
    "        dataset[:, -1][dataset[:, -1] == 3] = 1\n",
    "        dataset[:, -1][dataset[:, -1] == 5] = 1\n",
    "#         dataset[:, -1][dataset[:, -1] == 10] = 2\n",
    "        \n",
    "        # data segmentation\n",
    "        n_class = 2 #int(11-len(removed_label))  # 0~9 classes ('10:rest' is not considered)\n",
    "        no_feature = 64  # the number of the features\n",
    "        segment_length = 160 #160  # selected time window; 16=160*0.1\n",
    "        \n",
    "        #Overlapping is removed to avoid training set overlap with test set\n",
    "        data_seg = extract(dataset, n_classes=n_class, n_fea=no_feature, \n",
    "                           time_window=segment_length, moving=(segment_length))  # /2 for 50% overlapping\n",
    "        print('After segmentation, the shape of the data:', data_seg.shape)\n",
    "\n",
    "        # split training and test data\n",
    "        no_longfeature = no_feature*segment_length\n",
    "        data_seg_feature = data_seg[:, :no_longfeature]\n",
    "        self.data_seg_label = data_seg[:, no_longfeature:no_longfeature+1]\n",
    "        \n",
    "        # Its important to have random state set equal for Training and test dataset\n",
    "        train_feature, test_feature, train_label, test_label = train_test_split(\n",
    "            data_seg_feature, self.data_seg_label,random_state=0, shuffle=True,stratify=self.data_seg_label)\n",
    "\n",
    "        # Check the class label splits to maintain balance\n",
    "        unique, counts = np.unique(self.data_seg_label, return_counts=True)\n",
    "        left_perc = counts[0]/sum(counts)\n",
    "        if left_perc < 0.4 or left_perc > 0.6:\n",
    "            print(\"Imbalanced dataset with split of: \",left_perc,1-left_perc)\n",
    "        else:\n",
    "            print(\"Classes balanced.\")\n",
    "        unique, counts = np.unique(train_label, return_counts=True)\n",
    "        print(\"Class label splits in training set \\n \",np.asarray((unique, counts)).T)\n",
    "        unique, counts = np.unique(test_label, return_counts=True)\n",
    "        print(\"Class label splits in test set\\n \",np.asarray((unique, counts)).T)\n",
    "\n",
    "\n",
    "\n",
    "        # normalization\n",
    "        # before normalize reshape data back to raw data shape\n",
    "        train_feature_2d = train_feature.reshape([-1, no_feature])\n",
    "        test_feature_2d = test_feature.reshape([-1, no_feature])\n",
    "\n",
    "        scaler1 = StandardScaler().fit(train_feature_2d)\n",
    "        train_fea_norm1 = scaler1.transform(train_feature_2d) # normalize the training data\n",
    "        test_fea_norm1 = scaler1.transform(test_feature_2d) # normalize the test data\n",
    "        print('After normalization, the shape of training feature:', train_fea_norm1.shape,\n",
    "              '\\nAfter normalization, the shape of test feature:', test_fea_norm1.shape)\n",
    "        \n",
    "        # after normalization, reshape data to 3d\n",
    "        train_fea_norm1 = train_fea_norm1.reshape([-1, segment_length, no_feature])\n",
    "        test_fea_norm1 = test_fea_norm1.reshape([-1, segment_length, no_feature])\n",
    "        print('After reshape, the shape of training feature:', train_fea_norm1.shape,\n",
    "              '\\nAfter reshape, the shape of test feature:', test_fea_norm1.shape)\n",
    "        \n",
    "        # reshape for data shape: (trial, conv channel, electrode channel, time samples)\n",
    "        # earlier it was (trial,timesamples,electrode_channel)\n",
    "        train_fea_reshape1 = np.swapaxes(np.expand_dims(train_fea_norm1,1),2,3)\n",
    "        test_fea_reshape1 = np.swapaxes(np.expand_dims(test_fea_norm1,1),2,3)\n",
    "        print('After expand dims, the shape of training feature:', train_fea_reshape1.shape,\n",
    "              '\\nAfter expand dims, the shape of test feature:', test_fea_reshape1.shape)\n",
    "        \n",
    "        self.data = torch.tensor(train_fea_reshape1)\n",
    "        self.targets = torch.tensor(train_label.flatten()).long()\n",
    "        \n",
    "        print(\"data and target type:\",type(self.data),type(self.targets))\n",
    "\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data, target = self.data[idx], self.targets[idx]\n",
    "        return data, target\n",
    "    \n",
    "    def get_class_weights(self):\n",
    "        class_weights=class_weight.compute_class_weight('balanced',np.unique(self.data_seg_label),\n",
    "                                                        self.data_seg_label[:,0])\n",
    "        return class_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ef5445",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGMMIDTsSet(Data.Dataset):\n",
    "    def __init__(self, subject_id, transform=None):\n",
    "        root_dir = \"../Deep-Learning-for-BCI/dataset/\"\n",
    "#         dataset = np.load(root_dir + str(subject_id) + '.npy')\n",
    "        dataset_raw = np.load(root_dir + str(subject_id) + '.npy')\n",
    "        dataset=[]  # feature after filtering\n",
    "\n",
    "        # EEG Gamma pattern decomposition\n",
    "        for i in range(dataset_raw[:,:-1].shape[1]):\n",
    "            x = dataset_raw[:, i]\n",
    "            fs = 160.0\n",
    "            lowcut = 8.0\n",
    "            highcut = 30.0\n",
    "            y = butter_bandpass_filter(x, lowcut, highcut, fs, order=3)\n",
    "            dataset.append(y)\n",
    "        dataset=np.array(dataset).T\n",
    "        dataset=np.hstack((dataset,dataset_raw[:,-1:]))\n",
    "        # keep 4,5 which are left and right fist open close imagery classes, remove rest\n",
    "        # refer 1-Data.ipynb for the details\n",
    "        removed_label = [0,1,6,7,8,9,10]  # [0,1,2,3,4,5,10] for hf # [0,1,6,7,8,9,10] for lr\n",
    "        for ll in removed_label:\n",
    "            id = dataset[:, -1]!=ll\n",
    "            dataset = dataset[id]\n",
    "\n",
    "        # Pytorch needs labels to be sequentially ordered starting from 0\n",
    "        dataset[:, -1][dataset[:, -1] == 2] = 0\n",
    "        dataset[:, -1][dataset[:, -1] == 4] = 0\n",
    "        dataset[:, -1][dataset[:, -1] == 3] = 1\n",
    "        dataset[:, -1][dataset[:, -1] == 5] = 1\n",
    "#         dataset[:, -1][dataset[:, -1] == 10] = 2\n",
    "        \n",
    "        # data segmentation\n",
    "        n_class = 2 #int(11-len(removed_label))  # 0~9 classes ('10:rest' is not considered)\n",
    "        no_feature = 64  # the number of the features\n",
    "        segment_length = 160 #160  # selected time window; 16=160*0.1\n",
    "        \n",
    "        #Overlapping is removed to avoid training set overlap with test set\n",
    "        data_seg = extract(dataset, n_classes=n_class, n_fea=no_feature, \n",
    "                           time_window=segment_length, moving=(segment_length))  # /2 for 50% overlapping\n",
    "        print('After segmentation, the shape of the data:', data_seg.shape)\n",
    "\n",
    "        # split training and test data\n",
    "        no_longfeature = no_feature*segment_length\n",
    "        data_seg_feature = data_seg[:, :no_longfeature]\n",
    "        data_seg_label = data_seg[:, no_longfeature:no_longfeature+1]\n",
    "        # Its important to have random state set equal for Training and test dataset\n",
    "        train_feature, test_feature, train_label, test_label = train_test_split(\n",
    "            data_seg_feature, data_seg_label,random_state=0, shuffle=True,stratify=data_seg_label)\n",
    "\n",
    "        # Check the class label splits to maintain balance\n",
    "        unique, counts = np.unique(data_seg_label, return_counts=True)\n",
    "        left_perc = counts[0]/sum(counts)\n",
    "        if left_perc < 0.4 or left_perc > 0.6:\n",
    "            print(\"Imbalanced dataset with split of: \",left_perc,1-left_perc)\n",
    "        else:\n",
    "            print(\"Classes balanced.\")\n",
    "        unique, counts = np.unique(train_label, return_counts=True)\n",
    "        print(\"Class label splits in training set \\n \",np.asarray((unique, counts)).T)\n",
    "        unique, counts = np.unique(test_label, return_counts=True)\n",
    "        print(\"Class label splits in test set\\n \",np.asarray((unique, counts)).T)\n",
    "\n",
    "\n",
    "\n",
    "        # normalization\n",
    "        # before normalize reshape data back to raw data shape\n",
    "        train_feature_2d = train_feature.reshape([-1, no_feature])\n",
    "        test_feature_2d = test_feature.reshape([-1, no_feature])\n",
    "\n",
    "        scaler1 = StandardScaler().fit(train_feature_2d)\n",
    "        train_fea_norm1 = scaler1.transform(train_feature_2d) # normalize the training data\n",
    "        test_fea_norm1 = scaler1.transform(test_feature_2d) # normalize the test data\n",
    "        print('After normalization, the shape of training feature:', train_fea_norm1.shape,\n",
    "              '\\nAfter normalization, the shape of test feature:', test_fea_norm1.shape)\n",
    "\n",
    "        # after normalization, reshape data to 3d\n",
    "        train_fea_norm1 = train_fea_norm1.reshape([-1, segment_length, no_feature])\n",
    "        test_fea_norm1 = test_fea_norm1.reshape([-1, segment_length, no_feature])\n",
    "        print('After reshape, the shape of training feature:', train_fea_norm1.shape,\n",
    "              '\\nAfter reshape, the shape of test feature:', test_fea_norm1.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # reshape for data shape: (trial, conv channel, electrode channel, time samples)\n",
    "        # earlier it was (trial,timesamples,electrode_channel)\n",
    "        train_fea_reshape1 = np.swapaxes(np.expand_dims(train_fea_norm1,1),2,3)\n",
    "        test_fea_reshape1 = np.swapaxes(np.expand_dims(test_fea_norm1,1),2,3)\n",
    "        print('After expand dims, the shape of training feature:', train_fea_reshape1.shape,\n",
    "              '\\nAfter expand dims, the shape of test feature:', test_fea_reshape1.shape)\n",
    "        \n",
    "        self.data =  torch.tensor(test_fea_reshape1)\n",
    "        self.targets = torch.tensor(test_label.flatten()).long()\n",
    "        \n",
    "        print(\"data and target type:\",type(self.data),type(self.targets))\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data, target = self.data[idx], self.targets[idx]\n",
    "        return data, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a70d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution module\n",
    "# use conv to capture local features, instead of postion embedding.\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, emb_size=40):\n",
    "        # self.patch_size = patch_size\n",
    "        super().__init__()\n",
    "\n",
    "        self.shallownet = nn.Sequential(\n",
    "            nn.Conv2d(1, 40, (1, 25), (1, 1)),\n",
    "            nn.Conv2d(40, 40, (22, 1), (1, 1)), # 22 when using 64 channels # 7 for 21,19,18 channels\n",
    "            nn.BatchNorm2d(40),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, 75), (1, 15)),  # pooling acts as slicing to obtain 'patch' along the time dimension as in ViT\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(40, emb_size, (1, 1), stride=(1, 1)),  # transpose, conv could enhance fiting ability slightly\n",
    "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        b, _, _, _ = x.shape\n",
    "        x = self.shallownet(x.float())\n",
    "        x = self.projection(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        self.keys = nn.Linear(emb_size, emb_size)\n",
    "        self.queries = nn.Linear(emb_size, emb_size)\n",
    "        self.values = nn.Linear(emb_size, emb_size)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "\n",
    "    def forward(self, x: Tensor, mask: Tensor = None) -> Tensor:\n",
    "        queries = rearrange(self.queries(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        keys = rearrange(self.keys(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        values = rearrange(self.values(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)  \n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            energy.mask_fill(~mask, fill_value)\n",
    "\n",
    "        scaling = self.emb_size ** (1 / 2)\n",
    "        att = F.softmax(energy / scaling, dim=-1)\n",
    "        att = self.att_drop(att)\n",
    "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x\n",
    "\n",
    "\n",
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size, expansion, drop_p):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return input*0.5*(1.0+torch.erf(input/math.sqrt(2.0)))\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 emb_size,\n",
    "                 num_heads=10,\n",
    "                 drop_p=0.5,\n",
    "                 forward_expansion=4,\n",
    "                 forward_drop_p=0.5):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                MultiHeadAttention(emb_size, num_heads, drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                FeedForwardBlock(\n",
    "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )\n",
    "            ))\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, depth, emb_size):\n",
    "        super().__init__(*[TransformerEncoderBlock(emb_size) for _ in range(depth)])\n",
    "\n",
    "\n",
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, emb_size, n_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # global average pooling\n",
    "        self.clshead = nn.Sequential(\n",
    "            Reduce('b n e -> b e', reduction='mean'),\n",
    "            nn.LayerNorm(emb_size),\n",
    "            nn.Linear(emb_size, n_classes)\n",
    "        )\n",
    "        # 3000 for 21 channels 1 s # 2600 for top 19 channels # 2400,18\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(8600, 256), # 25800 for 2s, 8600 for 1s for 64 channels\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(32, n_classes) #4 # change here for classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "        out = self.fc(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Conformer(nn.Sequential):\n",
    "    def __init__(self, emb_size=40, depth=2, n_classes=2, **kwargs):\n",
    "        super().__init__(\n",
    "\n",
    "            PatchEmbedding(emb_size),\n",
    "            TransformerEncoder(depth, emb_size),\n",
    "            ClassificationHead(emb_size, n_classes)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a642c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_eegmmid(task_type, strat, sub_id, i=\"\"):\n",
    "    \n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    train_ds = EEGMMIDTrSet(subject_id=sub_id)\n",
    "    test_ds = EEGMMIDTsSet(subject_id=sub_id)\n",
    "    class_weights = torch.tensor(train_ds.get_class_weights(),dtype=torch.float,device=device)\n",
    "    scenario = ni_benchmark(train_dataset=train_ds,test_dataset=test_ds,\n",
    "                       n_experiences=5,task_labels=True)\n",
    "\n",
    "#     tb_logger = TensorboardLogger()\n",
    "    text_logger = TextLogger(open('eegmmidlog.txt', 'a'))\n",
    "#     int_logger = InteractiveLogger()\n",
    "\n",
    "    eval_plugin = EvaluationPlugin(\n",
    "        accuracy_metrics(minibatch=True, epoch=True, experience=True, stream=True),  # forward = - backward su streaming, stream Ã¨ la media\n",
    "        class_accuracy_metrics(epoch=True, stream=True, classes=list(range(scenario.n_classes))),\n",
    "        loss_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "        timing_metrics(epoch=True, epoch_running=True),\n",
    "        forgetting_metrics(experience=True, stream=True),\n",
    "        cpu_usage_metrics(experience=True),\n",
    "        gpu_usage_metrics(0, experience=True),\n",
    "        disk_usage_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "        loggers=[text_logger]\n",
    "    )\n",
    "\n",
    "    es = EarlyStoppingPlugin(patience=50, val_stream_name=\"train_stream\")\n",
    "    \n",
    "    # Changed learning rate and betas based on the parameters from Conformer paper\n",
    "    results = []\n",
    "    model = Conformer(n_classes=2).cuda()\n",
    "    if (strat == \"naive\"):\n",
    "        print(\"Naive continual learning\")\n",
    "        strategy = Naive(model, Adam(model.parameters(), lr=0.0002, betas=(0.5, 0.99)), CrossEntropyLoss(weight=class_weights), train_epochs=100, eval_every=10, plugins=[es], evaluator=eval_plugin, device=device)\n",
    "    elif (strat == \"offline\"):\n",
    "        print(\"Offline learning\")\n",
    "        strategy = JointTraining(model, Adam(model.parameters(), lr=0.0005, betas=(0.5, 0.99)), CrossEntropyLoss(weight=class_weights), train_epochs=1000, eval_every=10, plugins=[es], evaluator=eval_plugin, device=device, train_mb_size=25)\n",
    "    elif (strat == \"cumulative\"):\n",
    "        print(\"Cumulative continual learning\")\n",
    "        strategy = Cumulative(model, Adam(model.parameters(), lr=0.0002, betas=(0.5, 0.99)), CrossEntropyLoss(weight=class_weights), train_epochs=100, eval_every=10, plugins=[es], evaluator=eval_plugin, device=device,train_mb_size=25)\n",
    "    elif (strat == \"replay\"):\n",
    "        print(\"Replay training\")\n",
    "        strategy = Replay(model, Adam(model.parameters(), lr=0.0002, betas=(0.5, 0.99)), CrossEntropyLoss(weight=class_weights), train_epochs=100, eval_every=10, plugins=[es], evaluator=eval_plugin, device=device, mem_size=25, train_mb_size=25)  #circa 25% of ASCERTAIN\n",
    "    elif (strat == \"lwf\"):\n",
    "        print(\"LwF continual learning\")\n",
    "        strategy = LwF(model, Adam(model.parameters(), lr=0.0002, betas=(0.5, 0.99)), CrossEntropyLoss(weight=class_weights), train_epochs=100, eval_every=10, plugins=[es], evaluator=eval_plugin, device=device, alpha=0.5, temperature=1)\n",
    "    elif (strat == \"ewc\"):\n",
    "        print(\"EWC continual learning\")\n",
    "        torch.backends.cudnn.enabled = False\n",
    "        strategy = EWC(model, Adam(model.parameters(), lr=0.0002, betas=(0.5, 0.99)), CrossEntropyLoss(weight=class_weights), train_epochs=100, eval_every=10, plugins=[es], evaluator=eval_plugin, device=device, ewc_lambda=0.99)\n",
    "    elif (strat == \"episodic\"):\n",
    "        print(\"Episodic continual learning\")\n",
    "        strategy = GEM(model, Adam(model.parameters(), lr=0.0002, betas=(0.5, 0.99)), CrossEntropyLoss(weight=class_weights), train_epochs=1, eval_every=10, plugins=[es], evaluator=eval_plugin, device=device, patterns_per_exp=70)\n",
    "\n",
    "    thisresults = []\n",
    "\n",
    "    print(i + \".\")\n",
    "    start = time.time()\n",
    "    if strat == \"offline\":\n",
    "        res = strategy.train(scenario.train_stream)\n",
    "#         print(\"-------------Train-----------\")\n",
    "#         print(res)\n",
    "        r = strategy.eval(scenario.test_stream)\n",
    "#         print(\"-------------Test-----------\")\n",
    "#         print(r)\n",
    "        thisresults.append({\"task_type\":task_type,\n",
    "                            \"strategy\":strat,\n",
    "                            \"sub_id\":sub_id,\n",
    "                            \"iteration\":i,\n",
    "                            \"loss\":r[\"Loss_Exp/eval_phase/test_stream/Task000/Exp000\"],\n",
    "                            \"acc\":(float(r[\"Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp000\"])*100),\n",
    "                            \"acc0\":(float(r[\"Top1_ClassAcc_Stream/eval_phase/test_stream/Task000/0\"])*100),\n",
    "                            \"acc1\":(float(r[\"Top1_ClassAcc_Stream/eval_phase/test_stream/Task000/1\"])*100),\n",
    "#                             \"acc2\":(float(r[\"Top1_ClassAcc_Stream/eval_phase/test_stream/Task000/2\"])*100),\n",
    "                            \"forg\":r[\"StreamForgetting/eval_phase/test_stream\"],\n",
    "                            \"all\":r})\n",
    "        results.append({\"task_type\":task_type,\n",
    "                        \"strategy\":strat,\n",
    "                        \"sub_id\":sub_id,\n",
    "                        \"iteration\":i,\n",
    "                        \"finalloss\":r[\"Loss_Exp/eval_phase/test_stream/Task000/Exp000\"],\n",
    "                        \"finalacc\":r[\"Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp000\"],\n",
    "                        \"results\":thisresults})\n",
    "        torch.save(model.state_dict(), \"./results/eegconformer/eegmmid_ws_\" +strat +\"_\"+ str(sub_id)+ \"_model\" + i +'.pth')\n",
    "    else:\n",
    "        for experience in scenario.train_stream:\n",
    "            res = strategy.train(experience)\n",
    "            r = strategy.eval(scenario.test_stream)\n",
    "            thisresults.append({\"task_type\":task_type,\n",
    "                                \"strategy\":strat,\n",
    "                                \"sub_id\":sub_id,\n",
    "                                \"iteration\":i,\n",
    "                                \"loss\":r[\"Loss_Exp/eval_phase/test_stream/Task000/Exp000\"],\n",
    "                                \"acc\":(float(r[\"Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp000\"])*100),\n",
    "                                \"acc0\":(float(r[\"Top1_ClassAcc_Stream/eval_phase/test_stream/Task000/0\"])*100),\n",
    "                                \"acc1\":(float(r[\"Top1_ClassAcc_Stream/eval_phase/test_stream/Task000/1\"])*100),\n",
    "#                                 \"acc2\":(float(r[\"Top1_ClassAcc_Stream/eval_phase/test_stream/Task000/2\"])*100),\n",
    "                                \"forg\":r[\"StreamForgetting/eval_phase/test_stream\"],\n",
    "                                \"all\":r})\n",
    "        results.append({\"task_type\":task_type,\n",
    "                        \"strategy\":strat,\n",
    "                        \"sub_id\":sub_id,\n",
    "                        \"iteration\":i,\n",
    "                        \"finalloss\":r[\"Loss_Exp/eval_phase/test_stream/Task000/Exp000\"],\n",
    "                        \"finalacc\":r[\"Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp000\"],\n",
    "                        \"results\":thisresults})\n",
    "    elapsed = time.time() - start\n",
    "#     results.append({\"time\":elapsed})\n",
    "    with open(\"./results/eegconformer/eegmmid_ws_\" + strat +\"_\"+ str(sub_id)+ \"_results\" + i + \".pkl\", \"wb\") as outfile:\n",
    "        pickle.dump(results, outfile)\n",
    "    print(\"\\t\" + str(elapsed) + \" seconds\")\n",
    "\n",
    "for s_id in [7, 12, 22, 42, 43, 48, 49, 53, 70, 80, 82, 85, 94, 102]:\n",
    "    print(\"\\n --------------------------------------------------- \\n\")\n",
    "    print(\"Starting for subject id:\",s_id)\n",
    "    for itr in range(5):\n",
    "        train_eegmmid(task_type=\"within_sub\",strat=\"offline\", sub_id=s_id, i=str(itr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
