{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4babdca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "gpus = [1]\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join(map(str, gpus))\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import numpy as np\n",
    "import math\n",
    "import glob\n",
    "import random\n",
    "import itertools\n",
    "import datetime\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "import scipy.io\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "import torch.utils.data as Data\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary\n",
    "import torch.autograd as autograd\n",
    "from torchvision.models import vgg19\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn.init as init\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "# from common_spatial_pattern import csp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "from matplotlib import mlab as mlab\n",
    "from torch.backends import cudnn\n",
    "# from tSNE import plt_tsne\n",
    "# from grad_cam.utils import GradCAM, show_cam_on_image\n",
    "from utils import GradCAM, show_cam_on_image\n",
    "\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True\n",
    "\n",
    "import myimporter\n",
    "from BCI_functions import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import ot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dc8a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the overall model class, omitted here\n",
    "class ViT(nn.Sequential):\n",
    "    def __init__(self, emb_size=40, depth=2, n_classes=2, **kwargs):\n",
    "        super().__init__(\n",
    "            # ... the model\n",
    "        )\n",
    "        \n",
    "# ! A crucial step for adaptation on Transformer\n",
    "# reshape_transform  b 61 40 -> b 40 1 61\n",
    "def reshape_transform(tensor):\n",
    "    result = rearrange(tensor, 'b (h w) e -> b e (h) (w)', h=1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84acb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution module\n",
    "# use conv to capture local features, instead of postion embedding.\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, emb_size=40):\n",
    "        # self.patch_size = patch_size\n",
    "        super().__init__()\n",
    "\n",
    "        self.shallownet = nn.Sequential(\n",
    "            nn.Conv2d(1, 40, (1, 25), (1, 1)),\n",
    "            nn.Conv2d(40, 40, (22, 1), (1, 1)),\n",
    "            nn.BatchNorm2d(40),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, 75), (1, 15)),  # pooling acts as slicing to obtain 'patch' along the time dimension as in ViT\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(40, emb_size, (1, 1), stride=(1, 1)),  # transpose, conv could enhance fiting ability slightly\n",
    "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        b, _, _, _ = x.shape\n",
    "        x = self.shallownet(x.float())\n",
    "        x = self.projection(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        self.keys = nn.Linear(emb_size, emb_size)\n",
    "        self.queries = nn.Linear(emb_size, emb_size)\n",
    "        self.values = nn.Linear(emb_size, emb_size)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "\n",
    "    def forward(self, x: Tensor, mask: Tensor = None) -> Tensor:\n",
    "        queries = rearrange(self.queries(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        keys = rearrange(self.keys(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        values = rearrange(self.values(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)  \n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            energy.mask_fill(~mask, fill_value)\n",
    "\n",
    "        scaling = self.emb_size ** (1 / 2)\n",
    "        att = F.softmax(energy / scaling, dim=-1)\n",
    "        att = self.att_drop(att)\n",
    "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x\n",
    "\n",
    "\n",
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size, expansion, drop_p):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return input*0.5*(1.0+torch.erf(input/math.sqrt(2.0)))\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 emb_size,\n",
    "                 num_heads=10,\n",
    "                 drop_p=0.5,\n",
    "                 forward_expansion=4,\n",
    "                 forward_drop_p=0.5):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                MultiHeadAttention(emb_size, num_heads, drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                FeedForwardBlock(\n",
    "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )\n",
    "            ))\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, depth, emb_size):\n",
    "        super().__init__(*[TransformerEncoderBlock(emb_size) for _ in range(depth)])\n",
    "\n",
    "\n",
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, emb_size, n_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # global average pooling\n",
    "        self.clshead = nn.Sequential(\n",
    "            Reduce('b n e -> b e', reduction='mean'),\n",
    "            nn.LayerNorm(emb_size),\n",
    "            nn.Linear(emb_size, n_classes)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(8600, 256), # 25800 for 2s, 8600 for 1s # 2440 for bci\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(32, n_classes) #4 # change here for classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "        out = self.fc(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Conformer(nn.Sequential):\n",
    "    def __init__(self, emb_size=40, depth=2, n_classes=4, **kwargs):\n",
    "        super().__init__(\n",
    "\n",
    "            PatchEmbedding(emb_size),\n",
    "            TransformerEncoder(depth, emb_size),\n",
    "            ClassificationHead(emb_size, n_classes)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866d039a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: This class if has list of subject id can later support combination of sub ids\n",
    "# TODO: add a function transform to convert dataset to train test, avoiding repetition of same code\n",
    "\n",
    "class EEGMMIDTrSet(Data.Dataset):\n",
    "    def __init__(self, subject_id, transform=None):\n",
    "        root_dir = \"../Deep-Learning-for-BCI/dataset/\"\n",
    "        dataset = np.load(root_dir + str(subject_id) + '.npy')\n",
    "        # keep 4,5 which are left and right fist open close imagery classes, remove rest\n",
    "        # refer 1-Data.ipynb for the details\n",
    "        removed_label = [0,1,6,7,8,9,10]  # [0,1,2,3,4,5,10] for hf # [0,1,6,7,8,9,10] for lr\n",
    "        for ll in removed_label:\n",
    "            id = dataset[:, -1]!=ll\n",
    "            dataset = dataset[id]\n",
    "\n",
    "        # Pytorch needs labels to be sequentially ordered starting from 0\n",
    "        dataset[:, -1][dataset[:, -1] == 2] = 0\n",
    "        dataset[:, -1][dataset[:, -1] == 4] = 0\n",
    "        dataset[:, -1][dataset[:, -1] == 3] = 1\n",
    "        dataset[:, -1][dataset[:, -1] == 5] = 1\n",
    "#         dataset[:, -1][dataset[:, -1] == 10] = 2\n",
    "        \n",
    "        # data segmentation\n",
    "        n_class = 2 #int(11-len(removed_label))  # 0~9 classes ('10:rest' is not considered)\n",
    "        no_feature = 64  # the number of the features\n",
    "        segment_length = 160 #160  # selected time window; 16=160*0.1\n",
    "        \n",
    "        #Overlapping is removed to avoid training set overlap with test set\n",
    "        data_seg = extract(dataset, n_classes=n_class, n_fea=no_feature, \n",
    "                           time_window=segment_length, moving=(segment_length))  # /2 for 50% overlapping\n",
    "        print('After segmentation, the shape of the data:', data_seg.shape)\n",
    "\n",
    "        # split training and test data\n",
    "        no_longfeature = no_feature*segment_length\n",
    "        data_seg_feature = data_seg[:, :no_longfeature]\n",
    "        self.data_seg_label = data_seg[:, no_longfeature:no_longfeature+1]\n",
    "        \n",
    "        # Its important to have random state set equal for Training and test dataset\n",
    "        train_feature, test_feature, train_label, test_label = train_test_split(\n",
    "            data_seg_feature, self.data_seg_label,random_state=0, shuffle=True,stratify=self.data_seg_label)\n",
    "\n",
    "        # Check the class label splits to maintain balance\n",
    "        unique, counts = np.unique(self.data_seg_label, return_counts=True)\n",
    "        left_perc = counts[0]/sum(counts)\n",
    "        if left_perc < 0.4 or left_perc > 0.6:\n",
    "            print(\"Imbalanced dataset with split of: \",left_perc,1-left_perc)\n",
    "        else:\n",
    "            print(\"Classes balanced.\")\n",
    "        unique, counts = np.unique(train_label, return_counts=True)\n",
    "        print(\"Class label splits in training set \\n \",np.asarray((unique, counts)).T)\n",
    "        unique, counts = np.unique(test_label, return_counts=True)\n",
    "        print(\"Class label splits in test set\\n \",np.asarray((unique, counts)).T)\n",
    "\n",
    "\n",
    "\n",
    "        # normalization\n",
    "        # before normalize reshape data back to raw data shape\n",
    "        train_feature_2d = train_feature.reshape([-1, no_feature])\n",
    "        test_feature_2d = test_feature.reshape([-1, no_feature])\n",
    "\n",
    "        scaler1 = StandardScaler().fit(train_feature_2d)\n",
    "        train_fea_norm1 = scaler1.transform(train_feature_2d) # normalize the training data\n",
    "        test_fea_norm1 = scaler1.transform(test_feature_2d) # normalize the test data\n",
    "        print('After normalization, the shape of training feature:', train_fea_norm1.shape,\n",
    "              '\\nAfter normalization, the shape of test feature:', test_fea_norm1.shape)\n",
    "\n",
    "        # after normalization, reshape data to 3d\n",
    "        train_fea_norm1 = train_fea_norm1.reshape([-1, segment_length, no_feature])\n",
    "        test_fea_norm1 = test_fea_norm1.reshape([-1, segment_length, no_feature])\n",
    "        print('After reshape, the shape of training feature:', train_fea_norm1.shape,\n",
    "              '\\nAfter reshape, the shape of test feature:', test_fea_norm1.shape)\n",
    "        \n",
    "        # reshape for data shape: (trial, conv channel, electrode channel, time samples)\n",
    "        # earlier it was (trial,timesamples,electrode_channel)\n",
    "        train_fea_reshape1 = np.swapaxes(np.expand_dims(train_fea_norm1,1),2,3)\n",
    "        test_fea_reshape1 = np.swapaxes(np.expand_dims(test_fea_norm1,1),2,3)\n",
    "        print('After expand dims, the shape of training feature:', train_fea_reshape1.shape,\n",
    "              '\\nAfter expand dims, the shape of test feature:', test_fea_reshape1.shape)\n",
    "        \n",
    "        self.data = torch.tensor(train_fea_reshape1)\n",
    "        self.targets = torch.tensor(train_label.flatten()).long()\n",
    "        \n",
    "        print(\"data and target type:\",type(self.data),type(self.targets))\n",
    "\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data, target = self.data[idx], self.targets[idx]\n",
    "        return data, target\n",
    "    \n",
    "    def get_class_weights(self):\n",
    "        class_weights=class_weight.compute_class_weight('balanced',np.unique(self.data_seg_label),\n",
    "                                                        self.data_seg_label[:,0])\n",
    "        return class_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7658cd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "device = torch.device(\"cpu\")\n",
    "model = Conformer(n_classes=2)\n",
    "cat_dict = {0:'left hand movement',1:'right hand movement'}\n",
    "biosemi_montage = mne.channels.make_standard_montage('biosemi64')\n",
    "index = [8, 9, 10, 46, 45, 44, 43, 13, 12, 11, 47, 48, 49, 50, 16, 17, 18, \n",
    "         31, 55, 54, 53, 0, 32, 33, 1, 2, 36, 35, 34, 6, 5, 4, 3, 37, 38, \n",
    "         39, 40, 41, 7, 42, 14, 51, 23, 60, 15, 52, 22, 21, 20, 19, 30, 56, \n",
    "         57, 58, 59, 24, 25, 29, 62, 61, 26, 28, 63, 27]#range(64)#[37, 9, 10, 46, 45, 44, 13, 12, 11, 47, 48, 49, 50, 17, 18, 31, 55, 54, 19, 30, 56, 29]  # for bci competition iv 2a\n",
    "biosemi_montage.ch_names = [biosemi_montage.ch_names[i] for i in index]\n",
    "biosemi_montage.dig = [biosemi_montage.dig[i+3] for i in index]\n",
    "info = mne.create_info(ch_names=biosemi_montage.ch_names, sfreq=160,ch_types='eeg')\n",
    "\n",
    "top_channel_dict = {}\n",
    "\n",
    "for sub_id in [7, 12, 22, 42, 43, 48, 49, 53, 70, 80, 82, 85, 94, 102]:#range(1,11):\n",
    "    \n",
    "    model.load_state_dict(torch.load(\"../results/eegconformer/for_topfr/eegmmid_ws_offline_\"+ str(sub_id)+ \"_model0\" + '.pth', map_location=device)) \n",
    "    target_layers = [model[1]]  # set the target layer \n",
    "    cam = GradCAM(model=model, target_layers=target_layers, use_cuda=False, reshape_transform=reshape_transform)\n",
    "\n",
    "    train_ds = EEGMMIDTrSet(subject_id=sub_id)\n",
    "\n",
    "\n",
    "    data = train_ds.data\n",
    "    \n",
    "    \n",
    "    for target_category in range(2):\n",
    "\n",
    "        all_cam = []\n",
    "        # this loop is used to obtain the cam of each trial/sample\n",
    "        for i in range(data.shape[0]):\n",
    "            test = torch.as_tensor(data[i:i+1, :, :, :], dtype=torch.float32)\n",
    "            test = torch.autograd.Variable(test, requires_grad=True)\n",
    "            print(\"Ip shape: \",test.shape)\n",
    "\n",
    "            grayscale_cam = cam(input_tensor=test,target_category=target_category) #,target_category=2\n",
    "            grayscale_cam = grayscale_cam[0, :]\n",
    "            all_cam.append(grayscale_cam)\n",
    "\n",
    "        # the mean of all data\n",
    "        test_all_data = np.squeeze(np.mean(data.detach().cpu().numpy(), axis=0)) #.detach().cpu().numpy()\n",
    "        test_all_data = (test_all_data - np.mean(test_all_data)) / np.std(test_all_data)\n",
    "        mean_all_test = np.mean(test_all_data, axis=1)\n",
    "\n",
    "        # the mean of all cam\n",
    "        test_all_cam = np.mean(all_cam, axis=0)\n",
    "#         test_all_cam = (test_all_cam - np.mean(test_all_cam)) / np.std(test_all_cam)\n",
    "        mean_all_cam = np.mean(test_all_cam, axis=1)\n",
    "\n",
    "        # apply cam on the input data\n",
    "        hyb_all = test_all_data * test_all_cam\n",
    "        hyb_all = (hyb_all - np.mean(hyb_all)) / np.std(hyb_all)\n",
    "        mean_hyb_all = np.mean(hyb_all, axis=1)\n",
    "        \n",
    "        # get top 3 channel index -> feature relevance\n",
    "        nind = np.argsort(mean_hyb_all)[-10:]\n",
    "        print([biosemi_montage.ch_names[i] for i in nind])\n",
    "        \n",
    "        imp_ind = [mean_hyb_all[i] for i in nind]\n",
    "\n",
    "        ch_names = [biosemi_montage.ch_names[i] for i in nind]\n",
    "        print(imp_ind)\n",
    "        if not math.isnan(imp_ind[0]):\n",
    "            top_channel_dict[(sub_id,target_category)] = ch_names \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d552230f",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_list = []\n",
    "right_list = []\n",
    "for key in top_channel_dict:\n",
    "    if key[1] == 0:\n",
    "        left_list.extend(top_channel_dict[key])\n",
    "    elif key[1] == 1:\n",
    "        right_list.extend(top_channel_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cae91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter,OrderedDict\n",
    "freq_left = Counter(left_list)\n",
    "freq_right = Counter(right_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f75160",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(15, 6))\n",
    "plt.xticks(rotation = 45)\n",
    "ax.tick_params(axis='x', which='major')\n",
    "ax.set_xlabel('Channel Names', fontsize=10)\n",
    "ax.set_ylabel('Frequency in Top 10', fontsize=10)\n",
    "plt.bar(OrderedDict(freq_left.most_common()).keys(), OrderedDict(freq_left.most_common()).values())\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b673644b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(15, 6))\n",
    "# fig = plt.figure(figsize=(15,6))\n",
    "plt.xticks(rotation = 45)\n",
    "ax.tick_params(axis='x', which='major')\n",
    "ax.set_xlabel('Channel Names', fontsize=10)\n",
    "ax.set_ylabel('Frequency in Top 10', fontsize=10)\n",
    "plt.bar(OrderedDict(freq_right.most_common()).keys(), OrderedDict(freq_right.most_common()).values())\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d96a340",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(freq_left.most_common()[:11])\n",
    "print(freq_right.most_common()[:11])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2207145",
   "metadata": {},
   "source": [
    "## code to find float values corresponding to relevance based on freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a34fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "topfr_list = {}\n",
    "# Remove the last element from top 22 which has least freq if no overlaps\n",
    "for ch in ['CP2', 'Fp1', 'P6', 'AFz', 'C5', 'O2', 'FC6', 'F6', 'Fpz', 'FC4', 'AF8', 'AF7', 'P9', 'AF3', 'Fp2', 'F7', 'O1', 'F8', 'FT7', 'Oz', 'AF4']:\n",
    "    topfr_list[ch] = freq_left[ch] + freq_right[ch]\n",
    "print(topfr_list)\n",
    "assert len(topfr_list) ==21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04555f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_weight = 21/sum(topfr_list.values())\n",
    "print(unit_weight,sum(topfr_list.values()))\n",
    "\n",
    "for key in topfr_list:\n",
    "    topfr_list[key] = topfr_list[key] * unit_weight\n",
    "    \n",
    "print(sum(topfr_list.values()))\n",
    "topfr_list['T9'] = topfr_list.pop('P9')\n",
    "topfr_list['T10'] = topfr_list.pop('P10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbf1a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Replace 'your_file.csv' with the actual file path\n",
    "file_path = 'channel_loc_mat.csv'\n",
    "\n",
    "# Read CSV into a pandas DataFrame\n",
    "data_frame = pd.read_csv(file_path,header=None)\n",
    "\n",
    "# Extract values as a NumPy array\n",
    "topo_ref = data_frame.values\n",
    "\n",
    "# Display the NumPy array\n",
    "print(topo_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c96e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dimensions of the scalp grid (adjust as needed)\n",
    "rows, cols = 11,11\n",
    "\n",
    "# Create a 2D matrix to represent the location map\n",
    "topo_pred = np.zeros((rows, cols), dtype=float)\n",
    "for ch in topfr_list:\n",
    "    topo_pred = np.where(topo_ref == ch,topfr_list[ch] , topo_pred) #topfr_list[ch]\n",
    "print(sum(sum(topo_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5192a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(topo_pred)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858a7ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "topo_true = np.array([ [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "                       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "                       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "                       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "                       [0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
    "                       [0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
    "                       [0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
    "                       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "                       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "                       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "                       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
    "\n",
    "def compare_topomaps(topo_pred,topo_true):\n",
    "    \"\"\"\n",
    "    Show optimal transport on a moving disc in a 50x50 grid\n",
    "    \"\"\"\n",
    "    ## Step 1: Setup problem\n",
    "    pix = np.linspace(-1, 1, 11) # max channels are 13\n",
    "    # Setup grid\n",
    "    X, Y = np.meshgrid(pix, pix)\n",
    "    # Compute pariwise distances between points on 2D grid so we know\n",
    "    # how to score the Wasserstein distance\n",
    "    coords = np.array([X.flatten(), Y.flatten()]).T\n",
    "    coordsSqr = np.sum(coords**2, 1)\n",
    "    M = coordsSqr[:, None] + coordsSqr[None, :] - 2*coords.dot(coords.T)\n",
    "    M[M < 0] = 0\n",
    "    M = np.sqrt(M)\n",
    "    wass = ot.emd2(1e-5 +topo_pred.flatten(), 1e-5 +topo_true.flatten(), M, 1.0)\n",
    "    return wass\n",
    "\n",
    "compare_topomaps(topo_pred,topo_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d6c4c7",
   "metadata": {},
   "source": [
    "## Code to find top FR channels and then use them to generate a spatial map as in riemannian_allsubs_topFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f3e8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "top = [t[0] for t in freq_left.most_common()[:11]]\n",
    "\n",
    "top.extend([t[0] for t in freq_right.most_common()[:11]] )\n",
    "print(set(top))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec39ab28",
   "metadata": {},
   "outputs": [],
   "source": [
    "topfr_list = []\n",
    "# # Remove the last element from top 22 which has least freq\n",
    "for ch in ['Oz', 'C5', 'O2', 'F7', 'F6', 'AFz', 'AF3', 'Fp2', 'CP2', 'O1', 'AF4', 'Fpz', 'F8', 'FC6', 'FT7', 'AF7', 'P9', 'FC4', 'P6', 'AF8', 'Fp1']:\n",
    "    topfr_list.append(biosemi_montage.ch_names.index(ch))\n",
    "print(sorted(topfr_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d3b95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "biosemi_layout = mne.channels.read_layout(\"biosemi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c691141",
   "metadata": {},
   "outputs": [],
   "source": [
    "biosemi_layout.pos = np.asarray([biosemi_layout.pos[i] for i in index])\n",
    "biosemi_layout.names = [biosemi_layout.names[i] for i in index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0c9d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dimensions of the scalp grid (adjust as needed)\n",
    "rows, cols = 11,11\n",
    "\n",
    "# Create a 2D matrix to represent the location map\n",
    "location_map = np.zeros((rows, cols), dtype=float)\n",
    "\n",
    "# Define the positions of EEG sensors as 2D coordinates\n",
    "sensor_positions = biosemi_layout.pos[[5, 6, 7, 18, 21, 22, 23, 24, 25, 26, 27, 28, 29, 36, 37, 38, 42, 53, 60, 61, 62],:2]*11  # Replace with your actual sensor positions\n",
    "\n",
    "# Assign values in the matrix based on sensor positions\n",
    "for position in sensor_positions:\n",
    "    row, col = map(int, position)  # Convert coordinates to integers\n",
    "    location_map[row, col] = 1.0  # You can use different values for different sensors if needed\n",
    "\n",
    "# Now, location_map represents the 2D matrix of EEG sensor locations with floating-point coordinates\n",
    "location_map.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b2a129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dimensions of the scalp grid (adjust as needed)\n",
    "rows, cols = 11,11\n",
    "\n",
    "# Create a 2D matrix to represent the location map\n",
    "location_map = np.zeros((rows, cols), dtype=float)\n",
    "\n",
    "# Define the positions of EEG sensors as 2D coordinates\n",
    "sensor_positions = biosemi_layout.pos[:21,:2]*11  # Replace with your actual sensor positions\n",
    "\n",
    "# Assign values in the matrix based on sensor positions\n",
    "for position in sensor_positions:\n",
    "    row, col = map(int, position)  # Convert coordinates to integers\n",
    "    location_map[row, col] = 1.0  # You can use different values for different sensors if needed\n",
    "\n",
    "# Now, location_map represents the 2D matrix of EEG sensor locations with floating-point coordinates\n",
    "location_map.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b172f216",
   "metadata": {},
   "outputs": [],
   "source": [
    "topo_pred = np.array([ [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "                       [0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
    "                       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "                       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
    "                       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
    "                       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "                       [1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.],\n",
    "                       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
    "                       [0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.],\n",
    "                       [0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0.],\n",
    "                       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
    "\n",
    "topo_true = np.array([ [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "                       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "                       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "                       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "                       [0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
    "                       [0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
    "                       [0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
    "                       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "                       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "                       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "                       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bebd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(topo_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088e6bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(topo_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bc221e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(topo_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fd9057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_topomaps(topo_pred,topo_true):\n",
    "    \"\"\"\n",
    "    Show optimal transport on a moving disc in a 50x50 grid\n",
    "    \"\"\"\n",
    "    ## Step 1: Setup problem\n",
    "    pix = np.linspace(-1, 1, 11) # max channels are 13\n",
    "    # Setup grid\n",
    "    X, Y = np.meshgrid(pix, pix)\n",
    "    # Compute pariwise distances between points on 2D grid so we know\n",
    "    # how to score the Wasserstein distance\n",
    "    coords = np.array([X.flatten(), Y.flatten()]).T\n",
    "    coordsSqr = np.sum(coords**2, 1)\n",
    "    M = coordsSqr[:, None] + coordsSqr[None, :] - 2*coords.dot(coords.T)\n",
    "    M[M < 0] = 0\n",
    "    M = np.sqrt(M)\n",
    "    wass = ot.emd2(1e-5 +topo_pred.flatten(), 1e-5 +topo_true.flatten(), M, 1.0)\n",
    "    return wass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c6c711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "#     print(compare_topomaps(np.roll(topo_true, i,0),topo_true))\n",
    "\n",
    "compare_topomaps(topo_pred,topo_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5588c61",
   "metadata": {},
   "source": [
    "## The code for wasserstein distance comparison ends here. \n",
    "## Code for stat analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381f2043",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import fisher_exact\n",
    "\n",
    "# Example data\n",
    "model_correct = 10\n",
    "model_incorrect = 4\n",
    "baseline_correct = 8\n",
    "baseline_incorrect = 6\n",
    "\n",
    "# Create a 2x2 contingency table\n",
    "contingency_table = [[model_correct, model_incorrect], [baseline_correct, baseline_incorrect]]\n",
    "\n",
    "# Perform Fisher's exact test\n",
    "odds_ratio, p_value = fisher_exact(contingency_table, alternative='greater')\n",
    "\n",
    "# Set significance level (alpha)\n",
    "alpha = 0.05\n",
    "\n",
    "# Compare p-value to alpha\n",
    "if p_value < alpha:\n",
    "    print(f\"Reject the null hypothesis. Model accuracy is statistically significantly better than the baseline.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis. No significant difference in accuracy between the model and baseline.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a584732f",
   "metadata": {},
   "outputs": [],
   "source": [
    "70.97,68.82,69.15,69.89,68.82,82.80,69.89,73.12,73.12,70.97,70.97,77.42,68.82,72.04,82.80,72.04\n",
    "65.59,62.37,58.51,62.37,66.67,63.44,65.59,74.19,66.67,69.89,74.19,79.57,58.06,65.59,65.59,67.74\n",
    "77.42,65.59,70.21,70.97,73.12,80.65,63.44,69.89,68.82,70.97,78.49,68.82,69.89,60.22,75.27,69.89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482d59ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel, wilcoxon\n",
    "\n",
    "#Riem\n",
    "#r_all_channel_accuracies=[83.87,73.12,73.12,78.49,68.82,77.42,72.04,74.19,69.89,70.97,70.97,70.97,77.42,69.57]\n",
    "# mi_accuracies=[75.27,69.89,61.29,75.27,64.52,75.27,73.12,62.37,60.22,60.22,66.67,74.19,84.95,71.74]\n",
    "# topfr_acc = [74.19,73.12,63.44,77.42,61.29,72.04,65.59,73.12,67.74,64.52,67.74,65.59,75.27,58.70]\n",
    "\n",
    "# # EEGConformer\n",
    "c_all_channel_accuracies=[70.97,68.82,69.15,69.89,68.82,82.80,69.89,73.12,73.12,70.97,70.97,77.42,68.82,72.04,82.80,72.04]\n",
    "mi_accuracies=[77.42,65.59,70.21,70.97,73.12,80.65,63.44,69.89,68.82,70.97,78.49,68.82,69.89,60.22,75.27,69.89]\n",
    "topfr_acc=[65.59,62.37,58.51,62.37,66.67,63.44,65.59,74.19,66.67,69.89,74.19,79.57,58.06,65.59,65.59,67.74]\n",
    "\n",
    "\n",
    "# # EEGNet\n",
    "# topfr_acc = [66.67,62.37,56.99,56.99,59.14,64.52,73.12,65.59,61.29,51.61,63.44,63.44,62.37,59.78]\n",
    "# e_all_channel_accuracies = [78.49,65.59,59.14,68.82,63.44,70.97,75.27,68.82,59.14,60.22,62.37,70.97,77.42,57.61]\n",
    "# mi_accuracies = [81.72,58.06,61.29,78.49,58.06,63.44,73.12,64.52,58.06,59.14,54.84,65.59,78.49,60.87]\n",
    "\n",
    "# Perform a paired t-test\n",
    "t_stat, t_p_value = ttest_rel(topfr_acc, c_all_channel_accuracies)\n",
    "\n",
    "# Perform a Wilcoxon signed-rank test as a non-parametric alternative\n",
    "wilcoxon_stat, wilcoxon_p_value = wilcoxon(topfr_acc, c_all_channel_accuracies)\n",
    "\n",
    "# Set significance level (alpha)\n",
    "alpha = 0.05\n",
    "\n",
    "# Compare p-values to alpha\n",
    "if t_p_value < alpha:\n",
    "    print(f\"{t_p_value}: Paired t-test: Reject the null hypothesis. Model accuracies are statistically significantly better than the baseline.\")\n",
    "else:\n",
    "    print(f\"{t_p_value}: Paired t-test: Fail to reject the null hypothesis. No significant difference in accuracies between the model and baseline.\")\n",
    "\n",
    "if wilcoxon_p_value < alpha:\n",
    "    print(f\"{wilcoxon_p_value}: Wilcoxon signed-rank test: Reject the null hypothesis. Model accuracies are statistically significantly better than the baseline.\")\n",
    "else:\n",
    "    print(f\"{wilcoxon_p_value}: Wilcoxon signed-rank test: Fail to reject the null hypothesis. No significant difference in accuracies between the model and baseline.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
