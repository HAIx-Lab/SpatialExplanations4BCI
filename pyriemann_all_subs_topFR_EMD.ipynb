{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c32cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split,cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score,balanced_accuracy_score #roc_auc_score,precision_score,recall_score,f1_score,classification_report\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "import pyriemann\n",
    "import ot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "\n",
    "import myimporter\n",
    "from BCI_functions import *  # BCI_functions.ipynb contains some functions we might use multiple times in this tutorial\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b453688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: This class if has list of subject id can later support combination of sub ids\n",
    "# TODO: add a function transform to convert dataset to train test, avoiding repetition of same code\n",
    "\n",
    "class EEGMMIDTrSet(Data.Dataset):\n",
    "    def __init__(self, subject_id, transform=None):\n",
    "        root_dir = \"../Deep-Learning-for-BCI/dataset/\"\n",
    "        dataset_raw = np.load(root_dir + str(subject_id) + '.npy')\n",
    "        dataset=[]  # feature after filtering\n",
    "\n",
    "        # EEG Gamma pattern decomposition\n",
    "        for i in range(dataset_raw[:,:-1].shape[1]):\n",
    "            x = dataset_raw[:, i]\n",
    "            fs = 160.0\n",
    "            lowcut = 8.0\n",
    "            highcut = 30.0\n",
    "            y = butter_bandpass_filter(x, lowcut, highcut, fs, order=3)\n",
    "            dataset.append(y)\n",
    "        dataset=np.array(dataset).T\n",
    "        dataset=np.hstack((dataset,dataset_raw[:,-1:]))\n",
    "        print(dataset.shape)\n",
    "        # keep 4,5 which are left and right fist open close imagery classes, remove rest\n",
    "        # refer 1-Data.ipynb for the details\n",
    "        removed_label = [0,1,6,7,8,9,10]  # [0,1,2,3,4,5,10] for hf # [0,1,6,7,8,9,10] for lr\n",
    "        for ll in removed_label:\n",
    "            id = dataset[:, -1]!=ll\n",
    "            dataset = dataset[id]\n",
    "\n",
    "        # Pytorch needs labels to be sequentially ordered starting from 0\n",
    "        dataset[:, -1][dataset[:, -1] == 2] = 0\n",
    "        dataset[:, -1][dataset[:, -1] == 4] = 0\n",
    "        dataset[:, -1][dataset[:, -1] == 3] = 1\n",
    "        dataset[:, -1][dataset[:, -1] == 5] = 1\n",
    "#         dataset[:, -1][dataset[:, -1] == 10] = 2\n",
    "        \n",
    "        # data segmentation\n",
    "        n_class = 2 #int(11-len(removed_label))  # 0~9 classes ('10:rest' is not considered)\n",
    "        no_feature = 64  # the number of the features\n",
    "        segment_length = 160 #160  # selected time window; 16=160*0.1\n",
    "        \n",
    "        #Overlapping is removed to avoid training set overlap with test set\n",
    "        data_seg = extract(dataset, n_classes=n_class, n_fea=no_feature, \n",
    "                           time_window=segment_length, moving=(segment_length))  # /2 for 50% overlapping\n",
    "        print('After segmentation, the shape of the data:', data_seg.shape)\n",
    "\n",
    "        # split training and test data\n",
    "        no_longfeature = no_feature*segment_length\n",
    "        data_seg_feature = data_seg[:, :no_longfeature]\n",
    "        self.data_seg_label = data_seg[:, no_longfeature:no_longfeature+1]\n",
    "        \n",
    "        # Its important to have random state set equal for Training and test dataset\n",
    "        train_feature, test_feature, train_label, test_label = train_test_split(\n",
    "            data_seg_feature, self.data_seg_label,random_state=0, shuffle=True,stratify=self.data_seg_label)\n",
    "\n",
    "        # Check the class label splits to maintain balance\n",
    "        unique, counts = np.unique(self.data_seg_label, return_counts=True)\n",
    "        left_perc = counts[0]/sum(counts)\n",
    "        if left_perc < 0.4 or left_perc > 0.6:\n",
    "            print(\"Imbalanced dataset with split of: \",left_perc,1-left_perc)\n",
    "        else:\n",
    "            print(\"Classes balanced.\")\n",
    "        unique, counts = np.unique(train_label, return_counts=True)\n",
    "        print(\"Class label splits in training set \\n \",np.asarray((unique, counts)).T)\n",
    "        unique, counts = np.unique(test_label, return_counts=True)\n",
    "        print(\"Class label splits in test set\\n \",np.asarray((unique, counts)).T)\n",
    "\n",
    "\n",
    "\n",
    "        # normalization\n",
    "        # before normalize reshape data back to raw data shape\n",
    "        train_feature_2d = train_feature.reshape([-1, no_feature])\n",
    "        test_feature_2d = test_feature.reshape([-1, no_feature])\n",
    "\n",
    "        scaler1 = StandardScaler().fit(train_feature_2d)\n",
    "        train_fea_norm1 = scaler1.transform(train_feature_2d) # normalize the training data\n",
    "        test_fea_norm1 = scaler1.transform(test_feature_2d) # normalize the test data\n",
    "        print('After normalization, the shape of training feature:', train_fea_norm1.shape,\n",
    "              '\\nAfter normalization, the shape of test feature:', test_fea_norm1.shape)\n",
    "        \n",
    "        # after normalization, reshape data to 3d\n",
    "        train_fea_norm1 = train_fea_norm1.reshape([-1, segment_length, no_feature])\n",
    "        test_fea_norm1 = test_fea_norm1.reshape([-1, segment_length, no_feature])\n",
    "        print('After reshape, the shape of training feature:', train_fea_norm1.shape,\n",
    "              '\\nAfter reshape, the shape of test feature:', test_fea_norm1.shape)\n",
    "        \n",
    "        # reshape for data shape: (trial, conv channel, electrode channel, time samples)\n",
    "        # earlier it was (trial,timesamples,electrode_channel)\n",
    "        train_fea_reshape1 = np.swapaxes(train_fea_norm1,1,2)\n",
    "        test_fea_reshape1 = np.swapaxes(test_fea_norm1,1,2)\n",
    "        print('After expand dims, the shape of training feature:', train_fea_reshape1.shape,\n",
    "              '\\nAfter expand dims, the shape of test feature:', test_fea_reshape1.shape)\n",
    "        \n",
    "        self.data = train_fea_reshape1 # torch.tensor(train_fea_reshape1)\n",
    "        self.targets = train_label.flatten() #torch.tensor(train_label.flatten()).long()\n",
    "        \n",
    "        print(\"data and target type:\",type(self.data),type(self.targets))\n",
    "\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data, target = self.data[idx], self.targets[idx]\n",
    "        return data, target\n",
    "    \n",
    "    def get_class_weights(self):\n",
    "        class_weights=class_weight.compute_class_weight('balanced',np.unique(self.data_seg_label),\n",
    "                                                        self.data_seg_label[:,0])\n",
    "        return class_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ef5445",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGMMIDTsSet(Data.Dataset):\n",
    "    def __init__(self, subject_id, transform=None):\n",
    "        root_dir = \"../Deep-Learning-for-BCI/dataset/\"\n",
    "#         dataset = np.load(root_dir + str(subject_id) + '.npy')\n",
    "        dataset_raw = np.load(root_dir + str(subject_id) + '.npy')\n",
    "        dataset=[]  # feature after filtering\n",
    "\n",
    "        # EEG Gamma pattern decomposition\n",
    "        for i in range(dataset_raw[:,:-1].shape[1]):\n",
    "            x = dataset_raw[:, i]\n",
    "            fs = 160.0\n",
    "            lowcut = 8.0\n",
    "            highcut = 30.0\n",
    "            y = butter_bandpass_filter(x, lowcut, highcut, fs, order=3)\n",
    "            dataset.append(y)\n",
    "        dataset=np.array(dataset).T\n",
    "        dataset=np.hstack((dataset,dataset_raw[:,-1:]))\n",
    "        # keep 4,5 which are left and right fist open close imagery classes, remove rest\n",
    "        # refer 1-Data.ipynb for the details\n",
    "        removed_label = [0,1,6,7,8,9,10]  # [0,1,2,3,4,5,10] for hf # [0,1,6,7,8,9,10] for lr\n",
    "        for ll in removed_label:\n",
    "            id = dataset[:, -1]!=ll\n",
    "            dataset = dataset[id]\n",
    "\n",
    "        # Pytorch needs labels to be sequentially ordered starting from 0\n",
    "        dataset[:, -1][dataset[:, -1] == 2] = 0\n",
    "        dataset[:, -1][dataset[:, -1] == 4] = 0\n",
    "        dataset[:, -1][dataset[:, -1] == 3] = 1\n",
    "        dataset[:, -1][dataset[:, -1] == 5] = 1\n",
    "#         dataset[:, -1][dataset[:, -1] == 10] = 2\n",
    "        \n",
    "        # data segmentation\n",
    "        n_class = 2 #int(11-len(removed_label))  # 0~9 classes ('10:rest' is not considered)\n",
    "        no_feature = 64  # the number of the features\n",
    "        segment_length = 160 #160  # selected time window; 16=160*0.1\n",
    "        \n",
    "        #Overlapping is removed to avoid training set overlap with test set\n",
    "        data_seg = extract(dataset, n_classes=n_class, n_fea=no_feature, \n",
    "                           time_window=segment_length, moving=(segment_length))  # /2 for 50% overlapping\n",
    "        print('After segmentation, the shape of the data:', data_seg.shape)\n",
    "\n",
    "        # split training and test data\n",
    "        no_longfeature = no_feature*segment_length\n",
    "        data_seg_feature = data_seg[:, :no_longfeature]\n",
    "        data_seg_label = data_seg[:, no_longfeature:no_longfeature+1]\n",
    "        # Its important to have random state set equal for Training and test dataset\n",
    "        train_feature, test_feature, train_label, test_label = train_test_split(\n",
    "            data_seg_feature, data_seg_label,random_state=0, shuffle=True,stratify=data_seg_label)\n",
    "\n",
    "        # Check the class label splits to maintain balance\n",
    "        unique, counts = np.unique(data_seg_label, return_counts=True)\n",
    "        left_perc = counts[0]/sum(counts)\n",
    "        if left_perc < 0.4 or left_perc > 0.6:\n",
    "            print(\"Imbalanced dataset with split of: \",left_perc,1-left_perc)\n",
    "        else:\n",
    "            print(\"Classes balanced.\")\n",
    "        unique, counts = np.unique(train_label, return_counts=True)\n",
    "        print(\"Class label splits in training set \\n \",np.asarray((unique, counts)).T)\n",
    "        unique, counts = np.unique(test_label, return_counts=True)\n",
    "        print(\"Class label splits in test set\\n \",np.asarray((unique, counts)).T)\n",
    "\n",
    "\n",
    "\n",
    "        # normalization\n",
    "        # before normalize reshape data back to raw data shape\n",
    "        train_feature_2d = train_feature.reshape([-1, no_feature])\n",
    "        test_feature_2d = test_feature.reshape([-1, no_feature])\n",
    "\n",
    "        scaler1 = StandardScaler().fit(train_feature_2d)\n",
    "        train_fea_norm1 = scaler1.transform(train_feature_2d) # normalize the training data\n",
    "        test_fea_norm1 = scaler1.transform(test_feature_2d) # normalize the test data\n",
    "        print('After normalization, the shape of training feature:', train_fea_norm1.shape,\n",
    "              '\\nAfter normalization, the shape of test feature:', test_fea_norm1.shape)\n",
    "        \n",
    "\n",
    "        # after normalization, reshape data to 3d\n",
    "        train_fea_norm1 = train_fea_norm1.reshape([-1, segment_length, no_feature])\n",
    "        test_fea_norm1 = test_fea_norm1.reshape([-1, segment_length, no_feature])\n",
    "        print('After reshape, the shape of training feature:', train_fea_norm1.shape,\n",
    "              '\\nAfter reshape, the shape of test feature:', test_fea_norm1.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # reshape for data shape: (trial, conv channel, electrode channel, time samples)\n",
    "        # earlier it was (trial,timesamples,electrode_channel)\n",
    "        train_fea_reshape1 = np.swapaxes(train_fea_norm1,1,2)\n",
    "        test_fea_reshape1 = np.swapaxes(test_fea_norm1,1,2)\n",
    "        print('After expand dims, the shape of training feature:', train_fea_reshape1.shape,\n",
    "              '\\nAfter expand dims, the shape of test feature:', test_fea_reshape1.shape)\n",
    "        \n",
    "        self.data =  test_fea_reshape1#torch.tensor(test_fea_reshape1)\n",
    "        self.targets = test_label.flatten() #torch.tensor(test_label.flatten()).long()\n",
    "        \n",
    "        print(\"data and target type:\",type(self.data),type(self.targets))\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data, target = self.data[idx], self.targets[idx]\n",
    "        return data, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39d4aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_topFR_channels(sub_id,top_channel_dict):\n",
    "    \n",
    "    start = time.time()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    train_ds = EEGMMIDTrSet(subject_id=sub_id)\n",
    "#     test_ds = EEGMMIDTsSet(subject_id=sub_id)\n",
    "    \n",
    "    # compute covariance matrices on training data\n",
    "    cov_64 = pyriemann.estimation.Covariances('oas').transform(train_ds[:][0])\n",
    "    labels = train_ds[:][1]\n",
    "    \n",
    "    ES_instance = pyriemann.channelselection.ElectrodeSelection(nelec=21, metric='riemann', n_jobs=1)\n",
    "    ES_instance = ES_instance.fit(cov_64, y=labels, sample_weight=None)\n",
    "    # target category is -1 since no class specific feature relevance\n",
    "    top_channel_dict[(sub_id,-1)] = ES_instance.subelec_\n",
    "    top_channel_dict[(sub_id,'riem_distance')] = ES_instance.di_\n",
    "    print(\"Selected channels:\", ES_instance.subelec_)\n",
    "    print(f\"DI list of len {len(ES_instance.di_)}:\", ES_instance.di_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980d8e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = EEGMMIDTrSet(subject_id=1)\n",
    "cov_64 = pyriemann.estimation.Covariances('oas').transform(train_ds[:][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5ffaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_64.shape\n",
    "labels = train_ds[:][1]\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee6d500",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_channel_dict = {}\n",
    "for s_id in range(1,110):#[7,15,29,32,35,42,43,46,48,49,54,56,62,93,94,108]: #range(7,18):\n",
    "    print(\"\\n --------------------------------------------------- \\n\")\n",
    "    print(\"Starting for subject id:\",s_id)\n",
    "    for itr in range(1):\n",
    "        find_topFR_channels(sub_id=s_id,top_channel_dict=top_channel_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bc63c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./results/riem/eegmmid_ws_results_topchannels.pkl\", \"wb\") as outfile:\n",
    "        pickle.dump(top_channel_dict, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3a74d3",
   "metadata": {},
   "source": [
    "## Read top channel dict and regenerate importance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b149155",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./results/riem/eegmmid_ws_riem_topchannels.pkl\", \"rb\") as outfile:\n",
    "    top_channel_dict = pickle.load(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34907969",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_list = []\n",
    "right_list = []\n",
    "full_list=[]\n",
    "for key in top_channel_dict:\n",
    "    if key[0] in [7, 12, 22, 42, 43, 48, 49, 53, 70, 80, 82, 85, 94, 102]:\n",
    "        if key[1] == 0:\n",
    "            left_list.extend(top_channel_dict[key])\n",
    "        elif key[1] == 1:\n",
    "            right_list.extend(top_channel_dict[key])\n",
    "        elif key[1] == -1:\n",
    "            full_list.extend(top_channel_dict[key])\n",
    "\n",
    "\n",
    "from collections import Counter,OrderedDict\n",
    "# freq_left = Counter(left_list)\n",
    "# freq_right = Counter(right_list)\n",
    "\n",
    "freq_full = Counter(np.asarray(full_list))\n",
    "print(freq_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c51da0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 21 Feature relevant based on Riem distance back elimination\n",
    "print(sorted(list(OrderedDict(freq_full.most_common()).keys())[:21]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7130efb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common21 =freq_full.most_common()[:21]\n",
    "for ind in most_common21:\n",
    "    print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56df8bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "biosemi_montage = mne.channels.make_standard_montage('biosemi64')\n",
    "index = [8, 9, 10, 46, 45, 44, 43, 13, 12, 11, 47, 48, 49, 50, 16, 17, 18, \n",
    "         31, 55, 54, 53, 0, 32, 33, 1, 2, 36, 35, 34, 6, 5, 4, 3, 37, 38, \n",
    "         39, 40, 41, 7, 42, 14, 51, 23, 60, 15, 52, 22, 21, 20, 19, 30, 56, \n",
    "         57, 58, 59, 24, 25, 29, 62, 61, 26, 28, 63, 27]#range(64)#[37, 9, 10, 46, 45, 44, 13, 12, 11, 47, 48, 49, 50, 17, 18, 31, 55, 54, 19, 30, 56, 29]  # for bci competition iv 2a\n",
    "biosemi_montage.ch_names = [biosemi_montage.ch_names[i] for i in index]\n",
    "\n",
    "\n",
    "\n",
    "topfr_list = {}\n",
    "for ind in most_common21:\n",
    "    topfr_list[biosemi_montage.ch_names[ind[0]]] = ind[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198b7e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "topfr_list\n",
    "assert len(topfr_list) ==21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deec1418",
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_weight = 21/sum(topfr_list.values())\n",
    "print(unit_weight,sum(topfr_list.values()))\n",
    "\n",
    "for key in topfr_list:\n",
    "    topfr_list[key] = topfr_list[key] * unit_weight\n",
    "    \n",
    "print(sum(topfr_list.values()))\n",
    "topfr_list['T9'] = topfr_list.pop('P9')\n",
    "topfr_list['T10'] = topfr_list.pop('P10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5b8c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Replace 'your_file.csv' with the actual file path\n",
    "file_path = 'channel_loc_mat.csv'\n",
    "\n",
    "# Read CSV into a pandas DataFrame\n",
    "data_frame = pd.read_csv(file_path,header=None)\n",
    "\n",
    "# Extract values as a NumPy array\n",
    "topo_ref = data_frame.values\n",
    "\n",
    "# Display the NumPy array\n",
    "print(topo_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f2e211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dimensions of the scalp grid (adjust as needed)\n",
    "rows, cols = 11,11\n",
    "\n",
    "# Create a 2D matrix to represent the location map\n",
    "topo_pred = np.zeros((rows, cols), dtype=float)\n",
    "for ch in topfr_list:\n",
    "    topo_pred = np.where(topo_ref == ch,topfr_list[ch], topo_pred) #topfr_list[ch]\n",
    "print(sum(sum(topo_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f5e5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(topo_pred)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9a79ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "topo_true = np.array([ [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "                       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "                       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "                       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "                       [0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
    "                       [0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
    "                       [0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
    "                       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "                       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "                       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "                       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
    "\n",
    "def compare_topomaps(topo_pred,topo_true):\n",
    "    \"\"\"\n",
    "    Show optimal transport on a moving disc in a 50x50 grid\n",
    "    \"\"\"\n",
    "    ## Step 1: Setup problem\n",
    "    pix = np.linspace(-1, 1, 11) # max channels are 13\n",
    "    # Setup grid\n",
    "    X, Y = np.meshgrid(pix, pix)\n",
    "    # Compute pariwise distances between points on 2D grid so we know\n",
    "    # how to score the Wasserstein distance\n",
    "    coords = np.array([X.flatten(), Y.flatten()]).T\n",
    "    coordsSqr = np.sum(coords**2, 1)\n",
    "    M = coordsSqr[:, None] + coordsSqr[None, :] - 2*coords.dot(coords.T)\n",
    "    M[M < 0] = 0\n",
    "    M = np.sqrt(M)\n",
    "    wass = ot.emd2(1e-5 +topo_pred.flatten(), 1e-5 +topo_true.flatten(), M, 1.0)\n",
    "    return wass\n",
    "\n",
    "compare_topomaps(topo_pred,topo_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1661050",
   "metadata": {},
   "source": [
    "## Code for training on selected feature relevant channels from selected subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63dcc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: This class if has list of subject id can later support combination of sub ids\n",
    "# TODO: add a function transform to convert dataset to train test, avoiding repetition of same code\n",
    "\n",
    "class EEGMMIDTrSet(Data.Dataset):\n",
    "    def __init__(self, subject_id, transform=None):\n",
    "        root_dir = \"../Deep-Learning-for-BCI/dataset/\"\n",
    "        dataset_raw = np.load(root_dir + str(subject_id) + '.npy')\n",
    "        dataset=[]  # feature after filtering\n",
    "\n",
    "        # EEG Gamma pattern decomposition\n",
    "        for i in range(dataset_raw[:,:-1].shape[1]):\n",
    "            x = dataset_raw[:, i]\n",
    "            fs = 160.0\n",
    "            lowcut = 8.0\n",
    "            highcut = 30.0\n",
    "            y = butter_bandpass_filter(x, lowcut, highcut, fs, order=3)\n",
    "            dataset.append(y)\n",
    "        dataset=np.array(dataset).T\n",
    "        dataset=np.hstack((dataset,dataset_raw[:,-1:]))\n",
    "        print(dataset.shape)\n",
    "        # keep 4,5 which are left and right fist open close imagery classes, remove rest\n",
    "        # refer 1-Data.ipynb for the details\n",
    "        removed_label = [0,1,6,7,8,9,10]  # [0,1,2,3,4,5,10] for hf # [0,1,6,7,8,9,10] for lr\n",
    "        for ll in removed_label:\n",
    "            id = dataset[:, -1]!=ll\n",
    "            dataset = dataset[id]\n",
    "\n",
    "        # Pytorch needs labels to be sequentially ordered starting from 0\n",
    "        dataset[:, -1][dataset[:, -1] == 2] = 0\n",
    "        dataset[:, -1][dataset[:, -1] == 4] = 0\n",
    "        dataset[:, -1][dataset[:, -1] == 3] = 1\n",
    "        dataset[:, -1][dataset[:, -1] == 5] = 1\n",
    "#         dataset[:, -1][dataset[:, -1] == 10] = 2\n",
    "        \n",
    "        # data segmentation\n",
    "        n_class = 2 #int(11-len(removed_label))  # 0~9 classes ('10:rest' is not considered)\n",
    "        no_feature = 64  # the number of the features\n",
    "        segment_length = 160 #160  # selected time window; 16=160*0.1\n",
    "        \n",
    "        #Overlapping is removed to avoid training set overlap with test set\n",
    "        data_seg = extract(dataset, n_classes=n_class, n_fea=no_feature, \n",
    "                           time_window=segment_length, moving=(segment_length))  # /2 for 50% overlapping\n",
    "        print('After segmentation, the shape of the data:', data_seg.shape)\n",
    "\n",
    "        # split training and test data\n",
    "        no_longfeature = no_feature*segment_length\n",
    "        data_seg_feature = data_seg[:, :no_longfeature]\n",
    "        self.data_seg_label = data_seg[:, no_longfeature:no_longfeature+1]\n",
    "        \n",
    "        # Its important to have random state set equal for Training and test dataset\n",
    "        train_feature, test_feature, train_label, test_label = train_test_split(\n",
    "            data_seg_feature, self.data_seg_label,random_state=0, shuffle=True,stratify=self.data_seg_label)\n",
    "\n",
    "        # Check the class label splits to maintain balance\n",
    "        unique, counts = np.unique(self.data_seg_label, return_counts=True)\n",
    "        left_perc = counts[0]/sum(counts)\n",
    "        if left_perc < 0.4 or left_perc > 0.6:\n",
    "            print(\"Imbalanced dataset with split of: \",left_perc,1-left_perc)\n",
    "        else:\n",
    "            print(\"Classes balanced.\")\n",
    "        unique, counts = np.unique(train_label, return_counts=True)\n",
    "        print(\"Class label splits in training set \\n \",np.asarray((unique, counts)).T)\n",
    "        unique, counts = np.unique(test_label, return_counts=True)\n",
    "        print(\"Class label splits in test set\\n \",np.asarray((unique, counts)).T)\n",
    "\n",
    "\n",
    "\n",
    "        # normalization\n",
    "        # before normalize reshape data back to raw data shape\n",
    "        train_feature_2d = train_feature.reshape([-1, no_feature])\n",
    "        test_feature_2d = test_feature.reshape([-1, no_feature])\n",
    "\n",
    "        scaler1 = StandardScaler().fit(train_feature_2d)\n",
    "        train_fea_norm1 = scaler1.transform(train_feature_2d) # normalize the training data\n",
    "        test_fea_norm1 = scaler1.transform(test_feature_2d) # normalize the test data\n",
    "        print('After normalization, the shape of training feature:', train_fea_norm1.shape,\n",
    "              '\\nAfter normalization, the shape of test feature:', test_fea_norm1.shape)\n",
    "        \n",
    "        #list is to select topFR channels\n",
    "        train_fea_norm1 = train_fea_norm1[:,[0, 3, 6, 8, 12, 15, 23, 24, 25, 28, 30, 37, 39, 42, 43, 45, 52, 55, 59, 60, 63]]\n",
    "        test_fea_norm1 = test_fea_norm1[:,[0, 3, 6, 8, 12, 15, 23, 24, 25, 28, 30, 37, 39, 42, 43, 45, 52, 55, 59, 60, 63]]\n",
    "        no_feature = 21\n",
    "        \n",
    "        # after normalization, reshape data to 3d\n",
    "        train_fea_norm1 = train_fea_norm1.reshape([-1, segment_length, no_feature])\n",
    "        test_fea_norm1 = test_fea_norm1.reshape([-1, segment_length, no_feature])\n",
    "        print('After reshape, the shape of training feature:', train_fea_norm1.shape,\n",
    "              '\\nAfter reshape, the shape of test feature:', test_fea_norm1.shape)\n",
    "        \n",
    "        # reshape for data shape: (trial, conv channel, electrode channel, time samples)\n",
    "        # earlier it was (trial,timesamples,electrode_channel)\n",
    "        train_fea_reshape1 = np.swapaxes(train_fea_norm1,1,2)\n",
    "        test_fea_reshape1 = np.swapaxes(test_fea_norm1,1,2)\n",
    "        print('After expand dims, the shape of training feature:', train_fea_reshape1.shape,\n",
    "              '\\nAfter expand dims, the shape of test feature:', test_fea_reshape1.shape)\n",
    "        \n",
    "        self.data = train_fea_reshape1 # torch.tensor(train_fea_reshape1)\n",
    "        self.targets = train_label.flatten() #torch.tensor(train_label.flatten()).long()\n",
    "        \n",
    "        print(\"data and target type:\",type(self.data),type(self.targets))\n",
    "\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data, target = self.data[idx], self.targets[idx]\n",
    "        return data, target\n",
    "    \n",
    "    def get_class_weights(self):\n",
    "        class_weights=class_weight.compute_class_weight('balanced',np.unique(self.data_seg_label),\n",
    "                                                        self.data_seg_label[:,0])\n",
    "        return class_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55400da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGMMIDTsSet(Data.Dataset):\n",
    "    def __init__(self, subject_id, transform=None):\n",
    "        root_dir = \"../Deep-Learning-for-BCI/dataset/\"\n",
    "#         dataset = np.load(root_dir + str(subject_id) + '.npy')\n",
    "        dataset_raw = np.load(root_dir + str(subject_id) + '.npy')\n",
    "        dataset=[]  # feature after filtering\n",
    "\n",
    "        # EEG Gamma pattern decomposition\n",
    "        for i in range(dataset_raw[:,:-1].shape[1]):\n",
    "            x = dataset_raw[:, i]\n",
    "            fs = 160.0\n",
    "            lowcut = 8.0\n",
    "            highcut = 30.0\n",
    "            y = butter_bandpass_filter(x, lowcut, highcut, fs, order=3)\n",
    "            dataset.append(y)\n",
    "        dataset=np.array(dataset).T\n",
    "        dataset=np.hstack((dataset,dataset_raw[:,-1:]))\n",
    "        # keep 4,5 which are left and right fist open close imagery classes, remove rest\n",
    "        # refer 1-Data.ipynb for the details\n",
    "        removed_label = [0,1,6,7,8,9,10]  # [0,1,2,3,4,5,10] for hf # [0,1,6,7,8,9,10] for lr\n",
    "        for ll in removed_label:\n",
    "            id = dataset[:, -1]!=ll\n",
    "            dataset = dataset[id]\n",
    "\n",
    "        # Pytorch needs labels to be sequentially ordered starting from 0\n",
    "        dataset[:, -1][dataset[:, -1] == 2] = 0\n",
    "        dataset[:, -1][dataset[:, -1] == 4] = 0\n",
    "        dataset[:, -1][dataset[:, -1] == 3] = 1\n",
    "        dataset[:, -1][dataset[:, -1] == 5] = 1\n",
    "        \n",
    "        # data segmentation\n",
    "        n_class = 2 #int(11-len(removed_label))  # 0~9 classes ('10:rest' is not considered)\n",
    "        no_feature = 64  # the number of the features\n",
    "        segment_length = 160 #160  # selected time window; 16=160*0.1\n",
    "        \n",
    "        #Overlapping is removed to avoid training set overlap with test set\n",
    "        data_seg = extract(dataset, n_classes=n_class, n_fea=no_feature, \n",
    "                           time_window=segment_length, moving=(segment_length))  # /2 for 50% overlapping\n",
    "        print('After segmentation, the shape of the data:', data_seg.shape)\n",
    "\n",
    "        # split training and test data\n",
    "        no_longfeature = no_feature*segment_length\n",
    "        data_seg_feature = data_seg[:, :no_longfeature]\n",
    "        data_seg_label = data_seg[:, no_longfeature:no_longfeature+1]\n",
    "        # Its important to have random state set equal for Training and test dataset\n",
    "        train_feature, test_feature, train_label, test_label = train_test_split(\n",
    "            data_seg_feature, data_seg_label,random_state=0, shuffle=True,stratify=data_seg_label)\n",
    "\n",
    "        # Check the class label splits to maintain balance\n",
    "        unique, counts = np.unique(data_seg_label, return_counts=True)\n",
    "        left_perc = counts[0]/sum(counts)\n",
    "        if left_perc < 0.4 or left_perc > 0.6:\n",
    "            print(\"Imbalanced dataset with split of: \",left_perc,1-left_perc)\n",
    "        else:\n",
    "            print(\"Classes balanced.\")\n",
    "        unique, counts = np.unique(train_label, return_counts=True)\n",
    "        print(\"Class label splits in training set \\n \",np.asarray((unique, counts)).T)\n",
    "        unique, counts = np.unique(test_label, return_counts=True)\n",
    "        print(\"Class label splits in test set\\n \",np.asarray((unique, counts)).T)\n",
    "\n",
    "\n",
    "\n",
    "        # normalization\n",
    "        # before normalize reshape data back to raw data shape\n",
    "        train_feature_2d = train_feature.reshape([-1, no_feature])\n",
    "        test_feature_2d = test_feature.reshape([-1, no_feature])\n",
    "\n",
    "        scaler1 = StandardScaler().fit(train_feature_2d)\n",
    "        train_fea_norm1 = scaler1.transform(train_feature_2d) # normalize the training data\n",
    "        test_fea_norm1 = scaler1.transform(test_feature_2d) # normalize the test data\n",
    "        print('After normalization, the shape of training feature:', train_fea_norm1.shape,\n",
    "              '\\nAfter normalization, the shape of test feature:', test_fea_norm1.shape)\n",
    "        \n",
    "\n",
    "#         # list is to select topFR channels\n",
    "        train_fea_norm1 = train_fea_norm1[:,[0, 3, 6, 8, 12, 15, 23, 24, 25, 28, 30, 37, 39, 42, 43, 45, 52, 55, 59, 60, 63]]\n",
    "        test_fea_norm1 = test_fea_norm1[:,[0, 3, 6, 8, 12, 15, 23, 24, 25, 28, 30, 37, 39, 42, 43, 45, 52, 55, 59, 60, 63]]\n",
    "        no_feature = 21\n",
    "\n",
    "        # after normalization, reshape data to 3d\n",
    "        train_fea_norm1 = train_fea_norm1.reshape([-1, segment_length, no_feature])\n",
    "        test_fea_norm1 = test_fea_norm1.reshape([-1, segment_length, no_feature])\n",
    "        print('After reshape, the shape of training feature:', train_fea_norm1.shape,\n",
    "              '\\nAfter reshape, the shape of test feature:', test_fea_norm1.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # reshape for data shape: (trial, conv channel, electrode channel, time samples)\n",
    "        # earlier it was (trial,timesamples,electrode_channel)\n",
    "        train_fea_reshape1 = np.swapaxes(train_fea_norm1,1,2)\n",
    "        test_fea_reshape1 = np.swapaxes(test_fea_norm1,1,2)\n",
    "        print('After expand dims, the shape of training feature:', train_fea_reshape1.shape,\n",
    "              '\\nAfter expand dims, the shape of test feature:', test_fea_reshape1.shape)\n",
    "        \n",
    "        self.data =  test_fea_reshape1#torch.tensor(test_fea_reshape1)\n",
    "        self.targets = test_label.flatten() #torch.tensor(test_label.flatten()).long()\n",
    "        \n",
    "        print(\"data and target type:\",type(self.data),type(self.targets))\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data, target = self.data[idx], self.targets[idx]\n",
    "        return data, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a686e24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_acc(confusion_matrix, class_id):\n",
    "    \"\"\"\n",
    "    confusion matrix of multi-class classification\n",
    "    \n",
    "    class_id: id of a particular class \n",
    "    \n",
    "    \"\"\"\n",
    "    confusion_matrix = np.float64(confusion_matrix)\n",
    "    TP = confusion_matrix[class_id,class_id]\n",
    "    FN = np.sum(confusion_matrix[class_id]) - TP\n",
    "    FP = np.sum(confusion_matrix[:,class_id]) - TP\n",
    "    TN = np.sum(confusion_matrix) - TP - FN - FP\n",
    "    print(\"for class id: \",class_id)\n",
    "    print(f\"TP: {TP}, FN: {FN}, FP: {FP}, TN: {TN} \")\n",
    "    \n",
    "    # sensitivity = 0 if TP == 0\n",
    "    if TP != 0:\n",
    "        sensitivity = TP/(TP+FN)\n",
    "    else:\n",
    "        sensitivity = 0.\n",
    "    \n",
    "    specificity = TN/(TN+FP)\n",
    "    accuracy = (TP+TN)/(TP+FP+FN+TN)\n",
    "    \n",
    "    return sensitivity, specificity, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a642c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_eegmmid(task_type, strat, sub_id, i=\"\"):\n",
    "    \n",
    "    start = time.time()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    train_ds = EEGMMIDTrSet(subject_id=sub_id)\n",
    "    test_ds = EEGMMIDTsSet(subject_id=sub_id)\n",
    "    \n",
    "    # compute covariance matrices on training data\n",
    "    cov_topfr_21 = pyriemann.estimation.Covariances('oas').transform(train_ds[:][0])\n",
    "    labels = train_ds[:][1]\n",
    "    \n",
    "    mdm = pyriemann.classification.MDM(metric=dict(mean='riemann', distance='riemann'))\n",
    "    \n",
    "    # cross validation\n",
    "    cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    # Use scikit-learn Pipeline with cross_val_score function\n",
    "    scores = cross_val_score(mdm, cov_topfr_21, labels, cv=cv, n_jobs=1)\n",
    "    \n",
    "    \n",
    "    cov_topfr_21_test = pyriemann.estimation.Covariances('oas').transform(test_ds[:][0])\n",
    "    mdm = mdm.fit(cov_topfr_21, labels)\n",
    "    y_pred = mdm.predict(cov_topfr_21_test)\n",
    "    cm = confusion_matrix(test_ds[:][1], y_pred)\n",
    "    \n",
    "    acc_0 = get_class_acc(cm,0)\n",
    "    acc_1 = get_class_acc(cm,1)\n",
    "    acc = accuracy_score(test_ds[:][1], y_pred)\n",
    "\n",
    "    # Printing the results\n",
    "    class_balance = np.mean(labels == labels[0])\n",
    "    class_balance = max(class_balance, 1. - class_balance)\n",
    "    print(\"train acc: \",scores)\n",
    "    print(\"test acc: \",acc, acc_0, acc_1)\n",
    "    print(\"chance level acc: \",class_balance)\n",
    "    \n",
    "    results = []\n",
    "    thisresults = []\n",
    "\n",
    "    print(i + \".\")\n",
    "    \n",
    "    \n",
    "    thisresults.append({\"task_type\":task_type,\n",
    "                                \"strategy\":strat,\n",
    "                                \"sub_id\":sub_id,\n",
    "                                \"iteration\":i,\n",
    "                                \"chance level acc\":class_balance,\n",
    "                                \"acc\":acc,\n",
    "                                \"acc0\":acc_0[0],\n",
    "                                \"acc1\":acc_1[0] })\n",
    "    results.append({\"task_type\":task_type,\n",
    "                    \"strategy\":strat,\n",
    "                    \"sub_id\":sub_id,\n",
    "                    \"iteration\":i,\n",
    "                    \"results\":thisresults})\n",
    "    elapsed = time.time() - start\n",
    "#     results.append({\"time\":elapsed})\n",
    "    with open(\"./results/riem_topfr/eegmmid_ws_\" + strat +\"_\"+ str(sub_id)+ \"_results\" + i + \".pkl\", \"wb\") as outfile:\n",
    "        pickle.dump(results, outfile)\n",
    "    print(\"\\t\" + str(elapsed) + \" seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0f1730",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s_id in range(1,110):#[7,15,29,32,35,42,43,46,48,49,54,56,62,93,94,108]: #range(7,18):\n",
    "    print(\"\\n --------------------------------------------------- \\n\")\n",
    "    print(\"Starting for subject id:\",s_id)\n",
    "    for itr in range(1):\n",
    "        train_eegmmid(task_type=\"within_sub\",strat=\"riem_topfr\", sub_id=s_id, i=str(itr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703883d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 21 channels\n",
    "top_fr_ind = [0, 3, 6, 8, 12, 15, 23, 24, 25, 28, 30, 37, 39, 42, 43, 45, 52, 55, 59, 60, 63]\n",
    "import mne\n",
    "# load channel names as per sequence of data\n",
    "biosemi_montage = mne.channels.make_standard_montage('biosemi64')\n",
    "index = [8, 9, 10, 46, 45, 44, 43, 13, 12, 11, 47, 48, 49, 50, 16, 17, 18, \n",
    "         31, 55, 54, 53, 0, 32, 33, 1, 2, 36, 35, 34, 6, 5, 4, 3, 37, 38, \n",
    "         39, 40, 41, 7, 42, 14, 51, 23, 60, 15, 52, 22, 21, 20, 19, 30, 56, \n",
    "         57, 58, 59, 24, 25, 29, 62, 61, 26, 28, 63, 27]\n",
    "biosemi_montage.ch_names = [biosemi_montage.ch_names[i] for i in index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63f2bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_fr_ch_names = [biosemi_montage.ch_names[i] for i in top_fr_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65a972f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(top_fr_ch_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4055c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the dimensions of the scalp grid (adjust as needed)\n",
    "rows, cols = 11,11\n",
    "\n",
    "# Create a 2D matrix to represent the location map\n",
    "location_map = np.zeros((rows, cols), dtype=float)\n",
    "\n",
    "# Define the positions of EEG sensors as 2D coordinates\n",
    "sensor_positions = biosemi_layout.pos[:,:2]*11  # Replace with your actual sensor positions\n",
    "\n",
    "# Assign values in the matrix based on sensor positions\n",
    "for position in sensor_positions:\n",
    "    row, col = map(int, position)  # Convert coordinates to integers\n",
    "    location_map[row, col] += 1.0  # You can use different values for different sensors if needed\n",
    "\n",
    "# Now, location_map represents the 2D matrix of EEG sensor locations with floating-point coordinates\n",
    "location_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341e51ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8670baea",
   "metadata": {},
   "outputs": [],
   "source": [
    "coord = []\n",
    "for position in sensor_positions:\n",
    "    row, col = map(int, position)  # Convert coordinates to integers\n",
    "    coord.append((row, col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f286acda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# biosemi_layout.names[[0, 3, 6, 8, 12, 15, 23, 24, 25, 28, 30, 37, 39, 42, 43, 45, 52, 55, 59, 60, 63]]\n",
    "for ind in [0, 3, 6, 8, 12, 15, 23, 24, 25, 28, 30, 37, 39, 42, 43, 45, 52, 55, 59, 60, 63]:\n",
    "    print(biosemi_layout.names[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dd8829",
   "metadata": {},
   "outputs": [],
   "source": [
    "biosemi_layout.pos = np.asarray([biosemi_layout.pos[i] for i in index])\n",
    "biosemi_layout.names = [biosemi_layout.names[i] for i in index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d5714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dimensions of the scalp grid (adjust as needed)\n",
    "rows, cols = 11,11\n",
    "\n",
    "# Create a 2D matrix to represent the location map\n",
    "location_map = np.zeros((rows, cols), dtype=float)\n",
    "\n",
    "# Define the positions of EEG sensors as 2D coordinates\n",
    "sensor_positions = biosemi_layout.pos[[0, 3, 6, 8, 12, 15, 23, 24, 25, 28, 30, 37, 39, 42, 43, 45, 52, 55, 59, 60, 63],:2]*11  # Replace with your actual sensor positions\n",
    "\n",
    "# Assign values in the matrix based on sensor positions\n",
    "for position in sensor_positions:\n",
    "    row, col = map(int, position)  # Convert coordinates to integers\n",
    "    location_map[row, col] = 1.0  # You can use different values for different sensors if needed\n",
    "\n",
    "# Now, location_map represents the 2D matrix of EEG sensor locations with floating-point coordinates\n",
    "location_map.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cb477c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dimensions of the scalp grid (adjust as needed)\n",
    "rows, cols = 11,11\n",
    "\n",
    "# Create a 2D matrix to represent the location map\n",
    "location_map = np.zeros((rows, cols), dtype=float)\n",
    "\n",
    "# Define the positions of EEG sensors as 2D coordinates\n",
    "sensor_positions = biosemi_layout.pos[:21,:2]*11  # Replace with your actual sensor positions\n",
    "\n",
    "# Assign values in the matrix based on sensor positions\n",
    "for position in sensor_positions:\n",
    "    row, col = map(int, position)  # Convert coordinates to integers\n",
    "    location_map[row, col] = 1.0  # You can use different values for different sensors if needed\n",
    "\n",
    "# Now, location_map represents the 2D matrix of EEG sensor locations with floating-point coordinates\n",
    "location_map.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c457d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_positions.round()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
